# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dG_lSHcNGkNBaW9Os35MGPLkf54KCRCe
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import pickle, gzip
import copy
import math
from torchvision import datasets, transforms
from PIL import ImageOps
from torch.utils.data import ConcatDataset
import torch.utils.data as data_utils
from torch.autograd import Variable
from tqdm import tqdm
from IPython import display

class GAN_generator(nn.Module):
    def __init__(self, code_size, output_size):
        super(GAN_generator, self).__init__()
        #n_features = 100
        #n_out = 784
        self.generator_net =  nn.Sequential(
            nn.Linear(code_size, 256),
            nn.LeakyReLU(0.2),            
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, output_size)
        )

    def forward(self, x):
        return self.generator_net(x.cuda())
    
    
class GAN_discriminator(nn.Module):
    def __init__(self, input_size, output_size):
        super(GAN_discriminator, self).__init__()
        #n_features = 784
        #n_out = 1
        self.discriminator_net = nn.Sequential( 
            nn.Linear(input_size, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, output_size)
        )    

    def forward(self, x):
        return self.discriminator_net(x.cuda())

    
# Noise
def sample_noise_batch(batch_size, code_size):
    n = Variable(torch.randn(batch_size, code_size))
    return n.cuda() 
    
def images_to_vectors(images):
    return images.view(images.size(0), 784)

def vectors_to_images(vectors):
    return vectors.view(vectors.size(0), 1, 28, 28)

'''
def sample_data_batch(data, batch_size):
    idxs = np.random.choice(np.arange(data.shape[0]), size=batch_size)
    batch = torch.tensor(data[idxs], dtype=torch.float32)
    return batch.cuda()
'''

def generator_loss(generator, discriminator, noise):
    generated_data = generator(noise) #<generate data given noise>
    disc_on_generated_data = discriminator(generated_data) #<discriminator's opinion on generated data>
    logp_gen_is_real = F.logsigmoid(disc_on_generated_data)
    loss = -torch.mean(logp_gen_is_real, 0) #<generator loss. Mind the sign!>
    return loss

def discriminator_loss(discriminator, real_data, generated_data):
    disc_on_real_data = discriminator(real_data) #<discriminator's prediction on real data>
    disc_on_fake_data = discriminator(generated_data) #<discriminator's prediction on generated data>
    logp_real_is_real = F.logsigmoid(disc_on_real_data)
    logp_gen_is_fake = F.logsigmoid(- disc_on_fake_data)
    loss = -torch.mean(logp_real_is_real, 0) - torch.mean(logp_gen_is_fake, 0) #<discriminator loss>
    return loss

def sample_images(axs, generator, nrow, ncol, code_size, sharp=False):
    images = generator(sample_noise_batch(batch_size=nrow*ncol, code_size=code_size))
    images = images.data.cpu().numpy().reshape(nrow*ncol, 28, 28)
    if np.var(images)!=0:
        images = images.clip(np.min(train_set[0]),np.max(train_set[0]))
    for i in range(nrow*ncol):
        ax = axs[i // ncol][i % ncol]
        if sharp:
            ax.imshow(images[i], cmap="gray", interpolation="none")
        else:
            ax.imshow(images[i], cmap="gray")
        ax.xaxis.set_visible(False)
        ax.yaxis.set_visible(False)
        
    '''
        plt.subplot(nrow,ncol,i+1)
        if sharp:
            plt.imshow(images[i], interpolation="none")
        else:
            plt.imshow(images[i])
        
    plt.show()
    '''
def sample_probas(ax, discriminator, generator, batch_size, x_batch, code_size):
    plt.title('Generated vs real data')
    
    D_real = F.sigmoid(discriminator(x_batch))
    generated_data_batch = generator(sample_noise_batch(batch_size, code_size))
    D_fake = F.sigmoid(discriminator(generated_data_batch))
    
    ax.hist(D_real.data.cpu().numpy(),
             label='D(x)', alpha=0.5, range=[0,1])
    ax.hist(D_fake.data.cpu().numpy(),
             label='D(G(z))', alpha=0.5, range=[0,1])
    ax.legend(loc='best')
    #plt.show()

def GAN_train(discr_input, discr_output, gen_input, gen_output, batch_size, data_loader, n_epochs):
    d_losses = []
    g_losses = []
    max_g = 3
    axs = [[[] for _ in range(max_g)] for _ in range(2)]
    
    discriminator = GAN_discriminator(discr_input, discr_output)
    discriminator.cuda()
    generator = GAN_generator(gen_input, gen_output)
    generator.cuda()
    
    #optimizers
    disc_opt = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=[0.5, 0.999])
    gen_opt = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=[0.5, 0.999])
    
    for epoch in range(n_epochs):#tqdm(range(n_epochs)):
        if  len(g_losses) == 0 or (g_losses[-1] < d_losses[-1] * 16): #Hack? Yeah, it's hack.
            d_loss = []
            for i in range(10):
                # Train discriminator
                data_loader_iter = iter(data_loader)
                x_batch, y_batch = next(data_loader_iter)
                fake_data = generator(sample_noise_batch(batch_size, gen_input)) #gen_input==code_size
                disc_loss = discriminator_loss(discriminator, x_batch, fake_data)
                disc_opt.zero_grad()
                disc_loss.backward()
                disc_opt.step()
                d_loss.append(disc_loss.data.cpu().numpy()[0])

                
        if len(d_losses) > 0:
            d_losses.append(np.mean(d_loss) * 0.05 + d_losses[-1] * (1 - 0.05))
        else:
            d_losses.append(np.mean(d_loss))

        # Train generator
        noise = sample_noise_batch(batch_size, gen_input)
        gen_loss = generator_loss(generator, discriminator, noise)
        gen_opt.zero_grad()
        gen_loss.backward()
        gen_opt.step()
        
        if len(g_losses) > 0:
            g_losses.append(gen_loss.data.cpu().numpy()[0] * 0.05 + g_losses[-1] * (1 - 0.05))
        else:
            g_losses.append(gen_loss.data.cpu().numpy()[0])
    
        
        #plot results
        if epoch %100==0:
            display.clear_output(True)
            
            plt.figure(figsize=(18, 4))

            ax = plt.subplot2grid((2,3 + max_g + 2), (0,0), colspan=3, rowspan=2)
            ax.set_title("D-Loss")
            ax.set_xlabel("#iteration")
            ax.set_ylabel("loss")
            ax.plot(g_losses, 'r', label='G-loss', alpha=0.8)
            ax.plot(d_losses, 'b', label='D-loss', alpha=0.8)
            ax.legend()
            ax.grid(True)

            for i in range(0, 2):
                for j in range(3, 3 + max_g):
                    axs[i][j - 3] = plt.subplot2grid((2, 3 + max_g + 2), (i, j))
            sample_images(axs, generator, 2, max_g, gen_input, True)

            ax = plt.subplot2grid((2, 3 + max_g + 2), (0, 3 + max_g), colspan=2, rowspan=2)
            prev_hist = sample_probas(ax, discriminator, generator, 1000, x_batch, gen_input)
            plt.tight_layout()
            plt.show()

            '''
            sample_images(generator, 2, 3, gen_input, sharp=True)
            sample_probas(discriminator, generator, 1000, x_batch, gen_input)
            print('epoch {}'.format(epoch))
            print('dloss {}'.format(d_losses))
            print('gloss {}'.format(g_losses))
            '''

from google.colab import drive
drive.mount('/content/drive')
f = gzip.open('/content/drive/My Drive/Jupyter/BayesML_Burnaev/CL_project/mnist.pkl.gz', 'rb')
train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
f.close()

batch_size = 128
train_data = torch.Tensor(train_set[0])
train_data = train_data.view(len(train_data),-1)

#selecting number
filters = [np.where(train_set[1] == i) for i in range(10)]

train_loaders = [data_utils.DataLoader(data_utils.TensorDataset(train_data[filters[i]], 
                                                                float(i) * torch.ones(len(filters[i][0]))), 
                                       batch_size=batch_size, shuffle=True) for i in range(10)]

'''
batch_size = 128
transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Lambda(lambda x: x.view(784))])

# MNIST dataset
train_mnist = datasets.MNIST(root='./data/',
                                   train=True,
                                   transform=transform,
                                   download=True)

# Data loader
train_loader = data_utils.DataLoader(dataset=train_mnist,
                                          batch_size=batch_size, 
                                          shuffle=True)
'''

np.random.seed(12345)
def permuted_mnist(rand_state=12345, batch_size=32):
    rnd_permute = np.random.RandomState(rand_state)
    idx_permute = torch.from_numpy(rnd_permute.permutation(784)).type(torch.int64)
    transform = transforms.Compose([transforms.ToTensor(), 
                                    transforms.Lambda(lambda x: x.view(-1)[idx_permute].view(784))])
    train_loader = data_utils.DataLoader(datasets.MNIST('data', train=True, download=True,
                   transform=transform), batch_size=batch_size, shuffle=True)
    test_loader = data_utils.DataLoader(datasets.MNIST('data', train=False, download=True,
                   transform=transform), batch_size=batch_size, shuffle=True)
    return train_loader, test_loader

train_loader, test_loader = permuted_mnist()



GAN_train(discr_input=784, discr_output=1, gen_input=100, gen_output=784, batch_size=batch_size, data_loader=train_loaders[9], n_epochs=300)

