{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VCL_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThohcPefPSRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import gzip\n",
        "import pickle\n",
        "import sys\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4IZCJw2Pd6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "81660ee8-3afb-420f-f0f8-0c313c023d2f"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhIZwL3JPg3v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "936c0db4-4234-43e3-8303-e529f61efc26"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eda0k_MKPihI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesLinear(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(BayesLinear, self).__init__()\n",
        "    \n",
        "    self.inp = input_size\n",
        "    self.out = output_size\n",
        "    \n",
        "    self.w_mu = nn.Parameter(1e-6 + torch.zeros(self.out, self.inp)).to(device)\n",
        "    self.w_var = nn.Parameter(0.1 * torch.ones(self.out, self.inp))\n",
        "    self.w = torch.distributions.normal.Normal(self.w_mu, 1)\n",
        "    \n",
        "    self.w_prior_mu = torch.Tensor([0.]).to(device)\n",
        "    self.w_prior_var = torch.Tensor([1.]).to(device)    \n",
        "    \n",
        "  def forward(self, x, sampling=False, calculate_log_probs=False):\n",
        "    \n",
        "    if self.training or sampling:\n",
        "      w = self.w.sample()\n",
        "    else:\n",
        "      w = self.w_mu\n",
        "        \n",
        "    return F.linear(x, w)\n",
        "  \n",
        "  def loss_layer(self, x):\n",
        "    \n",
        "    cte_term = -0.5 * np.log(2 * np.pi)\n",
        "    det_sig_term = -torch.log(self.w_var)\n",
        "    inner = (x - self.w_mu) / self.w_var\n",
        "    dist_term = -0.5 * (inner**2)    \n",
        "    out = (cte_term + det_sig_term + dist_term).sum()\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def kl(self):\n",
        "    \n",
        "    const_term = -0.5 * self.inp * self.out\n",
        "    log_std_diff = 0.5 * torch.sum(self.w_prior_var - self.w_var)\n",
        "    mu_diff_term = 0.5 * torch.sum((torch.exp(self.w_var) + (self.w_prior_mu - self.w_mu)**2) / self.w_prior_var)    \n",
        "    kl_layer = const_term + log_std_diff + mu_diff_term\n",
        "    \n",
        "    return kl_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW_rm5SPPyhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VCL(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(new_VCL, self).__init__()\n",
        "    \n",
        "    self.inp = input_size\n",
        "    self.hid = hidden_size\n",
        "    self.out = output_size\n",
        "    \n",
        "    self.bfc1 = BayesLinear(self.inp, self.hid)\n",
        "    self.bfc2 = BayesLinear(self.hid, self.hid)\n",
        "    self.bfc3 = BayesLinear(self.hid, self.out)\n",
        "    \n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "  def forward(self, x, sampling=False):\n",
        "    \n",
        "    x = self.relu(self.bfc1(x, sampling=sampling))\n",
        "    x = self.relu(self.bfc2(x, sampling=sampling))\n",
        "    out = self.softmax(self.bfc3(x, sampling=sampling))\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def logpred(self, predictions, targets):\n",
        "    \n",
        "    error = nn.CrossEntropyLoss(reduction='mean') \n",
        "    log_lik = - torch.mean(error(predictions, torch.max(targets, 1)[1])) \n",
        "    \n",
        "    return log_lik\n",
        "  \n",
        "  def kl_calc(self):\n",
        "    \n",
        "    return self.bfc1.kl() + self.bfc2.kl() + self.bfc3.kl()  \n",
        "  \n",
        "  def train(self, x, y, batch_size, n_epochs, task_id, display_epoch=20):\n",
        "  \n",
        "    print('Task is:', task_id)\n",
        "    optim = torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "    N = x.shape[0]\n",
        "    if batch_size > N:\n",
        "      batch_size = N\n",
        "    \n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "      perm_inds = list(range(x.shape[0]))\n",
        "      np.random.shuffle(perm_inds)\n",
        "      cur_x = x[perm_inds]\n",
        "      cur_y = y[perm_inds]     \n",
        "      avg_cost = 0.\n",
        "      total_batch = int(np.ceil(N * 1.0 / batch_size))\n",
        "      \n",
        "      for i in range(total_batch):\n",
        "\n",
        "        start_ind = i*batch_size\n",
        "        end_ind = np.min([(i+1)*batch_size, N])\n",
        "        batch_x = cur_x[start_ind:end_ind, :]\n",
        "        batch_y = cur_y[start_ind:end_ind, :]        \n",
        "        batch_x = Variable(torch.from_numpy(batch_x)).to(device)\n",
        "        batch_y = Variable(torch.from_numpy(batch_y)).to(device)        \n",
        "        optim.zero_grad()   \n",
        "        outputs = self.forward(batch_x)       \n",
        "        log_pred = self.logpred(outputs, batch_y)\n",
        "        kl = self.kl_calc()\n",
        "        loss = kl / batch_y.shape[0] - log_pred \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        avg_cost += loss.item() / total_batch\n",
        "\n",
        "      if epoch % display_epoch == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost =\", \\\n",
        "                \"{:.9f}\".format(avg_cost))\n",
        "\n",
        "      costs.append(avg_cost)\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    return costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65pSWTOHQCH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_vcl(hidden_size, no_epochs, data_gen, coreset_method, coreset_size=0, batch_size=None, single_head=True):\n",
        "  \n",
        "  in_dim, out_dim = data_gen.get_dims()\n",
        "  x_coresets, y_coresets = [], []\n",
        "  x_testsets, y_testsets = [], []\n",
        "  \n",
        "  all_acc = np.array([])\n",
        "  \n",
        "  for task_id in range(data_gen.max_iter):\n",
        "    \n",
        "    x_train, y_train, x_test, y_test = data_gen.next_task()\n",
        "    x_testsets.append(x_test)\n",
        "    y_testsets.append(y_test)\n",
        "    \n",
        "    head = 0 if single_head else task_id\n",
        "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
        "    \n",
        "    if task_id == 0:\n",
        "      \n",
        "      ml_model = VCL(in_dim, hidden_size, output_size=10).to(device)  \n",
        "      ml_model.train(x_train, y_train, bsize, no_epochs, task_id)\n",
        "      torch.save(ml_model.state_dict(), 'my_model.pth')\n",
        "    \n",
        "    else: \n",
        "    \n",
        "      ml_model = VCL(in_dim, hidden_size, output_size=10).to(device)\n",
        "      ml_model.load_state_dict(torch.load('my_model.pth'))\n",
        "      ml_model.bfc3 = BayesLinear(hidden_size, 10).to(device)\n",
        "      ml_model.train(x_train, y_train, bsize, no_epochs, task_id)\n",
        "      torch.save(ml_model.state_dict(), 'my_model.pth')\n",
        "    \n",
        "    if coreset_size > 0:\n",
        "      x_coresets, y_coresets, x_train, y_train = coreset_method(x_coresets, y_coresets, x_train, y_train, coreset_size)\n",
        "    \n",
        "    acc = get_scores(ml_model, x_testsets, y_testsets, x_coresets, y_coresets, hidden_size, no_epochs, single_head, batch_size)\n",
        "    all_acc = concatenate_results(acc, all_acc)\n",
        "    \n",
        "  return all_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu3TZyZ1QHDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_coresets(x_coresets, y_coresets):\n",
        "  \n",
        "    merged_x, merged_y = x_coresets[0], y_coresets[0]\n",
        "    for i in range(1, len(x_coresets)):\n",
        "        merged_x = np.vstack((merged_x, x_coresets[i]))\n",
        "        merged_y = np.vstack((merged_y, y_coresets[i]))\n",
        "        \n",
        "    return merged_x, merged_y\n",
        "\n",
        "def get_scores(model, x_testsets, y_testsets, x_coresets, y_coresets, hidden_size, no_epochs, single_head, batch_size=None):\n",
        "    \n",
        "    torch.save(model.state_dict(), 'model.pth')\n",
        "    acc = []\n",
        "\n",
        "    if single_head:\n",
        "      if len(x_coresets) > 0:\n",
        "        x_train, y_train = merge_coresets(x_coresets, y_coresets)\n",
        "        bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
        "        final_model = VCL(x_train.shape[1], hidden_size, y_train.shape[1]).to(device)\n",
        "        final_model.load_state_dict(torch.load('model.pth'))\n",
        "        final_model.train(x_train, y_train, bsize, no_epochs, task_id=0)\n",
        "        torch.save(model.state_dict(), 'model.pth')\n",
        "      else:\n",
        "        final_model = model\n",
        "\n",
        "    for i in range(len(x_testsets)):\n",
        "      if not single_head:\n",
        "        if len(x_coresets) > 0:\n",
        "          x_train, y_train = x_coresets[i], y_coresets[i]\n",
        "          bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
        "          final_model = VCL(x_train.shape[1], hidden_size, y_train.shape[1]).to(device)\n",
        "          final_model.load_state_dict(torch.load('model.pth'))\n",
        "          final_model.train(x_train, y_train, bsize, no_epochs, task_id=i)\n",
        "          torch.save(model.state_dict(), 'model.pth')\n",
        "        else:\n",
        "          final_model = model\n",
        "\n",
        "      head = 0 if single_head else i\n",
        "      x_test, y_test = x_testsets[i], y_testsets[i]\n",
        "\n",
        "      x_test = Variable(torch.from_numpy(x_test)).to(device)\n",
        "\n",
        "      pred = final_model.forward(x_test)\n",
        "      pred_y = np.argmax(pred.detach().cpu().numpy(), axis=1)\n",
        "      y = np.argmax(y_test, axis=1)\n",
        "      cur_acc = len(np.where((pred_y - y) == 0)[0]) * 1.0 / y.shape[0]\n",
        "      acc.append(cur_acc)\n",
        "        \n",
        "    return acc\n",
        "\n",
        "def concatenate_results(score, all_score):\n",
        "  \n",
        "    if all_score.size == 0:\n",
        "        all_score = np.reshape(score, (1,-1))\n",
        "    else:\n",
        "        new_arr = np.empty((all_score.shape[0], all_score.shape[1]+1))\n",
        "        new_arr[:] = np.nan\n",
        "        new_arr[:,:-1] = all_score\n",
        "        all_score = np.vstack((new_arr, score))\n",
        "        \n",
        "    return all_score\n",
        "\n",
        "def plot(vcl, rand_vcl):\n",
        "  \n",
        "    plt.rc('text', usetex=False)\n",
        "    plt.rc('font', family='serif')\n",
        "\n",
        "    plt.figure(figsize=(7,3))\n",
        "    plt.plot(np.arange(len(vcl))+1, vcl, label='VCL', marker='o')\n",
        "    plt.plot(np.arange(len(rand_vcl))+1, rand_vcl, label='VCL + Random Coreset', marker='o')\n",
        "    plt.xticks(range(1, len(vcl)+1))\n",
        "    plt.ylabel('Average accuracy')\n",
        "    plt.xlabel('# tasks')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt3thGj7QTSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rand_from_batch(x_coreset, y_coreset, x_train, y_train, coreset_size):\n",
        "  \n",
        "    # Randomly select from (x_train, y_train) and add to current coreset (x_coreset, y_coreset)\n",
        "    idx = np.random.choice(x_train.shape[0], coreset_size, False)\n",
        "    x_coreset.append(x_train[idx,:])\n",
        "    y_coreset.append(y_train[idx,:])\n",
        "    x_train = np.delete(x_train, idx, axis=0)\n",
        "    y_train = np.delete(y_train, idx, axis=0)\n",
        "    \n",
        "    return x_coreset, y_coreset, x_train, y_train\n",
        "\n",
        "def update_distance(dists, x_train, current_id):\n",
        "  \n",
        "    for i in range(x_train.shape[0]):\n",
        "        current_dist = np.linalg.norm(x_train[i,:]-x_train[current_id,:])\n",
        "        dists[i] = np.minimum(current_dist, dists[i])\n",
        "        \n",
        "    return dists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dEGhAwaQcoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PermutedMnistGenerator():\n",
        "  def __init__(self, max_iter=10):\n",
        "\n",
        "    f = gzip.open('/content/drive/My Drive/mnist.pkl.gz', 'rb')\n",
        "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "    f.close()\n",
        "\n",
        "    self.X_train = np.vstack((train_set[0], valid_set[0]))\n",
        "    self.Y_train = np.hstack((train_set[1], valid_set[1]))\n",
        "    self.X_test = test_set[0]\n",
        "    self.Y_test = test_set[1]\n",
        "    self.max_iter = max_iter\n",
        "    self.cur_iter = 0\n",
        "\n",
        "  def get_dims(self):\n",
        "\n",
        "    return self.X_train.shape[1], 10\n",
        "\n",
        "  def next_task(self):\n",
        "\n",
        "    if self.cur_iter >= self.max_iter:\n",
        "        raise Exception('Number of tasks exceeded!')\n",
        "    else:\n",
        "        np.random.seed(self.cur_iter)\n",
        "        perm_inds = list(range(self.X_train.shape[1])) #\n",
        "        np.random.shuffle(perm_inds)\n",
        "\n",
        "        # Retrieve train data\n",
        "        next_x_train = deepcopy(self.X_train)\n",
        "        next_x_train = next_x_train[:,perm_inds]\n",
        "        next_y_train = np.eye(10)[self.Y_train]\n",
        "\n",
        "        # Retrieve test data\n",
        "        next_x_test = deepcopy(self.X_test)\n",
        "        next_x_test = next_x_test[:,perm_inds]\n",
        "        next_y_test = np.eye(10)[self.Y_test]\n",
        "\n",
        "        self.cur_iter += 1\n",
        "\n",
        "        return next_x_train, next_y_train, next_x_test, next_y_test\n",
        "\n",
        "hidden_size = 100\n",
        "batch_size = 256\n",
        "no_epochs = 100\n",
        "single_head = True\n",
        "num_tasks = 10\n",
        "\n",
        "# Run vanilla VCL\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 0\n",
        "data_gen = PermutedMnistGenerator(num_tasks)\n",
        "vcl_result = run_vcl(hidden_size, no_epochs, data_gen, \n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print('VCL', vcl_result)\n",
        "\n",
        "\n",
        "# Run random coreset VCL\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 200\n",
        "data_gen = PermutedMnistGenerator(num_tasks)\n",
        "rand_vcl_result = run_vcl(hidden_size, no_epochs, data_gen, \n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print('Random coreset VCL', rand_vcl_result)\n",
        "\n",
        "# Plot average accuracy\n",
        "vcl_avg = np.nanmean(vcl_result, 1)\n",
        "rand_vcl_avg = np.nanmean(rand_vcl_result, 1)\n",
        "plot(vcl_avg, rand_vcl_avg)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}