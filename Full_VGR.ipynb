{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full_VGR",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG1b16-I2fo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pickle, gzip\n",
        "import copy\n",
        "import math\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import ImageOps\n",
        "from torch.utils.data import ConcatDataset\n",
        "import torch.utils.data as data_utils\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "from IPython import display\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0El19VCr4Hlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "code_size= 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hasKlXPu6MjA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84973f08-ed62-4122-cebc-ffaee84fe7b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTAVaAoZ6UG-",
        "colab_type": "text"
      },
      "source": [
        "# Permuted MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUQI9slY6Wp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PermutedMnistGenerator():\n",
        "    def __init__(self, max_iter=10, random_seed=0):\n",
        "        # Open data file\n",
        "        f = gzip.open('/content/drive/My Drive/Jupyter/BayesML_Burnaev/CL_project/mnist.pkl.gz', 'rb')\n",
        "        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "        f.close()\n",
        "\n",
        "        # Define train and test data\n",
        "        self.X_train = np.vstack((train_set[0], valid_set[0]))\n",
        "        self.Y_train = np.hstack((train_set[1], valid_set[1]))\n",
        "        self.X_test = test_set[0]\n",
        "        self.Y_test = test_set[1]\n",
        "        self.random_seed = random_seed\n",
        "        self.max_iter = max_iter\n",
        "        self.cur_iter = 0\n",
        "\n",
        "        self.out_dim = 10           # Total number of unique classes\n",
        "        self.class_list = range(10) # List of unique classes being considered, in the order they appear\n",
        "\n",
        "        # self.classes is the classes (with correct indices for training/testing) of interest at each task_id\n",
        "        self.classes = []\n",
        "        for iter in range(self.max_iter):\n",
        "            self.classes.append(range(0,10))\n",
        "\n",
        "        self.sets = self.classes\n",
        "\n",
        "    def get_dims(self):\n",
        "        # Get data input and output dimensions\n",
        "        return self.X_train.shape[1], self.out_dim\n",
        "\n",
        "    def next_task(self):\n",
        "        if self.cur_iter >= self.max_iter:\n",
        "            raise Exception('Number of tasks exceeded!')\n",
        "        else:\n",
        "            np.random.seed(self.cur_iter+self.random_seed)\n",
        "            perm_inds = list(range(self.X_train.shape[1]))\n",
        "            # First task is (unpermuted) MNIST, subsequent tasks are random permutations of pixels\n",
        "            if self.cur_iter > 0:\n",
        "                np.random.shuffle(perm_inds)\n",
        "\n",
        "            # Retrieve train data\n",
        "            next_x_train = deepcopy(self.X_train)\n",
        "            next_x_train = next_x_train[:,perm_inds]\n",
        "\n",
        "            # Initialise next_y_train to zeros, then change relevant entries to ones, and then stack\n",
        "            next_y_train = np.zeros((len(next_x_train), 10))\n",
        "            next_y_train[:,0:10] = np.eye(10)[self.Y_train]\n",
        "\n",
        "            # Retrieve test data\n",
        "            next_x_test = deepcopy(self.X_test)\n",
        "            next_x_test = next_x_test[:,perm_inds]\n",
        "\n",
        "            next_y_test = np.zeros((len(next_x_test), 10))\n",
        "            next_y_test[:,0:10] = np.eye(10)[self.Y_test]\n",
        "\n",
        "            self.cur_iter += 1\n",
        "\n",
        "            return next_x_train, next_y_train, next_x_test, next_y_test\n",
        "\n",
        "    def reset(self):\n",
        "        self.cur_iter = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mzjzB6q26-H",
        "colab_type": "text"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1394DqbL2x3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN_generator(nn.Module):\n",
        "    def __init__(self, code_size, output_size):\n",
        "        super(GAN_generator, self).__init__()\n",
        "        #n_features = 100\n",
        "        #n_out = 784\n",
        "        self.generator_net =  nn.Sequential(\n",
        "            nn.Linear(code_size, 256),\n",
        "            nn.LeakyReLU(0.2),            \n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.generator_net(x.cuda())\n",
        "    \n",
        "    \n",
        "class GAN_discriminator(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(GAN_discriminator, self).__init__()\n",
        "        #n_features = 784\n",
        "        #n_out = 1\n",
        "        self.discriminator_net = nn.Sequential( \n",
        "            nn.Linear(input_size, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, output_size)\n",
        "        )    \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.discriminator_net(x.cuda())\n",
        "\n",
        "    \n",
        "# Noise\n",
        "def sample_noise_batch(batch_size, code_size):\n",
        "    n = Variable(torch.randn(batch_size, code_size))\n",
        "    return n.cuda() \n",
        "    \n",
        "def images_to_vectors(images):\n",
        "    return images.view(images.size(0), 784)\n",
        "\n",
        "def vectors_to_images(vectors):\n",
        "    return vectors.view(vectors.size(0), 1, 28, 28) \n",
        "\n",
        "def generator_loss(generator, discriminator, noise):\n",
        "    generated_data = generator(noise)\n",
        "    disc_on_generated_data = discriminator(generated_data)\n",
        "    logp_gen_is_real = F.logsigmoid(disc_on_generated_data)\n",
        "    loss = -torch.mean(logp_gen_is_real, 0)\n",
        "    return loss\n",
        "\n",
        "def discriminator_loss(discriminator, real_data, generated_data):\n",
        "    disc_on_real_data = discriminator(real_data)\n",
        "    disc_on_fake_data = discriminator(generated_data)\n",
        "    logp_real_is_real = F.logsigmoid(disc_on_real_data)\n",
        "    logp_gen_is_fake = F.logsigmoid(- disc_on_fake_data)\n",
        "    loss = -torch.mean(logp_real_is_real, 0) - torch.mean(logp_gen_is_fake, 0)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-UBhCWL29zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_images(axs, generator, nrow, ncol, code_size, sharp=False):\n",
        "    images = generator(sample_noise_batch(batch_size=nrow*ncol, code_size=code_size))\n",
        "    images = images.data.cpu().numpy().reshape(nrow*ncol, 28, 28)\n",
        "    if np.var(images)!=0:\n",
        "        images = images.clip(0., 1.)#np.min(train_set[0]),np.max(train_set[0]))\n",
        "    for i in range(nrow*ncol):\n",
        "        ax = axs[i // ncol][i % ncol]\n",
        "        if sharp:\n",
        "            ax.imshow(images[i], cmap=\"gray\", interpolation=\"none\")\n",
        "        else:\n",
        "            ax.imshow(images[i], cmap=\"gray\")\n",
        "        ax.xaxis.set_visible(False)\n",
        "        ax.yaxis.set_visible(False)\n",
        "        \n",
        "    '''\n",
        "        plt.subplot(nrow,ncol,i+1)\n",
        "        if sharp:\n",
        "            plt.imshow(images[i], interpolation=\"none\")\n",
        "        else:\n",
        "            plt.imshow(images[i])\n",
        "        \n",
        "    plt.show()\n",
        "    '''\n",
        "    return \n",
        "def sample_probas(ax, discriminator, generator, batch_size, x_batch, code_size):\n",
        "    plt.title('Generated vs real data')\n",
        "    \n",
        "    D_real = torch.sigmoid(discriminator(x_batch))\n",
        "    generated_data_batch = generator(sample_noise_batch(batch_size, code_size))\n",
        "    D_fake = torch.sigmoid(discriminator(generated_data_batch))\n",
        "    \n",
        "    ax.hist(D_real.data.cpu().numpy(),\n",
        "             label='D(x)', alpha=0.5, range=[0,1])\n",
        "    ax.hist(D_fake.data.cpu().numpy(),\n",
        "             label='D(G(z))', alpha=0.5, range=[0,1])\n",
        "    ax.legend(loc='best')\n",
        "    #plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbhDuRt03dyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GAN_train(discr_input, discr_output, gen_input, gen_output, batch_size, data_loader, n_epochs):\n",
        "    d_losses = []\n",
        "    g_losses = []\n",
        "    max_g = 3\n",
        "    axs = [[[] for _ in range(max_g)] for _ in range(2)]\n",
        "    \n",
        "    discriminator = GAN_discriminator(discr_input, discr_output)\n",
        "    discriminator.cuda()\n",
        "    generator = GAN_generator(gen_input, gen_output)\n",
        "    generator.cuda()\n",
        "    \n",
        "    #optimizers\n",
        "    disc_opt = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=[0.5, 0.999])\n",
        "    gen_opt = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=[0.5, 0.999])\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        if  len(g_losses) == 0 or (g_losses[-1] < d_losses[-1] * 16): #Hack? Yeah, it's hack.\n",
        "            d_loss = []\n",
        "            for i in range(10):\n",
        "                # Train discriminator\n",
        "                data_loader_iter = iter(data_loader)\n",
        "                x_batch, y_batch = next(data_loader_iter)\n",
        "                fake_data = generator(sample_noise_batch(batch_size, gen_input)) #gen_input==code_size\n",
        "                disc_loss = discriminator_loss(discriminator, x_batch, fake_data)\n",
        "                disc_opt.zero_grad()\n",
        "                disc_loss.backward()\n",
        "                disc_opt.step()\n",
        "                d_loss.append(disc_loss.data.cpu().numpy()[0])\n",
        "\n",
        "                \n",
        "        if len(d_losses) > 0:\n",
        "            d_losses.append(np.mean(d_loss) * 0.05 + d_losses[-1] * (1 - 0.05))\n",
        "        else:\n",
        "            d_losses.append(np.mean(d_loss))\n",
        "\n",
        "        # Train generator\n",
        "        noise = sample_noise_batch(batch_size, gen_input)\n",
        "        gen_loss = generator_loss(generator, discriminator, noise)\n",
        "        gen_opt.zero_grad()\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "        \n",
        "        if len(g_losses) > 0:\n",
        "            g_losses.append(gen_loss.data.cpu().numpy()[0] * 0.05 + g_losses[-1] * (1 - 0.05))\n",
        "        else:\n",
        "            g_losses.append(gen_loss.data.cpu().numpy()[0])\n",
        "    \n",
        "        \n",
        "        #plot results\n",
        "        if epoch %100==0:\n",
        "           \n",
        "            #display.clear_output(True)\n",
        "            '''\n",
        "            plt.figure(figsize=(18, 4))\n",
        "\n",
        "            ax = plt.subplot2grid((2,3 + max_g + 2), (0,0), colspan=3, rowspan=2)\n",
        "            ax.set_title(\"D-Loss\")\n",
        "            ax.set_xlabel(\"#iteration\")\n",
        "            ax.set_ylabel(\"loss\")\n",
        "            ax.plot(g_losses, 'r', label='G-loss', alpha=0.8)\n",
        "            ax.plot(d_losses, 'b', label='D-loss', alpha=0.8)\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "\n",
        "            for i in range(0, 2):\n",
        "                for j in range(3, 3 + max_g):\n",
        "                    axs[i][j - 3] = plt.subplot2grid((2, 3 + max_g + 2), (i, j))\n",
        "            sample_images(axs, generator, 2, max_g, gen_input, True)\n",
        "\n",
        "            ax = plt.subplot2grid((2, 3 + max_g + 2), (0, 3 + max_g), colspan=2, rowspan=2)\n",
        "            prev_hist = sample_probas(ax, discriminator, generator, 1000, x_batch, gen_input)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            \n",
        "            sample_images(generator, 2, 3, gen_input, sharp=True)\n",
        "            sample_probas(discriminator, generator, 1000, x_batch, gen_input)\n",
        "            '''\n",
        "            print('epoch {}\\n'.format(epoch))\n",
        "            print('dlosses {}\\n'.format(d_losses))\n",
        "            print('glosses {}\\n'.format(g_losses))\n",
        "\n",
        "    return generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOs1fD084MtI",
        "colab_type": "text"
      },
      "source": [
        "# Solver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uwHZtSx39qM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        \n",
        "        # Weight parameters 1-dim params for each layer\n",
        "        self.weight_mu = nn.Parameter(torch.zeros(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(1e-6 * torch.ones(out_features, in_features))\n",
        "        self.weight = torch.distributions.Normal(self.weight_mu, self.weight_sigma)\n",
        "        \n",
        "        # Prior distribution\n",
        "        self.weight_prior_mu = torch.Tensor([0.]).to(device) #torch.zeros(in_features)\n",
        "        self.weight_prior_sigma = torch.Tensor([1.]).to(device) #torch.ones(in_features)\n",
        "        #self.weight_prior = torch.distributions.Normal(self.weight_prior_mu, self.weight_prior_sigma)\n",
        "        \n",
        "        self.log_prior = 0.\n",
        "        self.log_variational_posterior = 0.\n",
        "\n",
        "    def forward(self, input, sampling=False, calculate_log_probs=False):\n",
        "        if self.training or sampling:\n",
        "            weight = self.weight.sample()\n",
        "        else:\n",
        "            weight = self.weight.mean\n",
        "        '''\n",
        "        if self.training or calculate_log_probs:\n",
        "            self.log_prior = self.weight_prior.log_prob(weight)\n",
        "            self.log_variational_posterior = self.weight.log_prob(weight)\n",
        "        else:\n",
        "            self.log_prior, self.log_variational_posterior = 0., 0.\n",
        "        '''\n",
        "        return F.linear(input, weight)\n",
        "    \n",
        "    def kl(self):\n",
        "        return (torch.log(torch.sqrt(self.weight_prior_sigma)) - torch.log(torch.sqrt(self.weight_sigma)) + (\n",
        "            (self.weight_mu - self.weight_prior_mu) ** 2 + self.weight_sigma - self.weight_prior_sigma) / (\n",
        "            2. * self.weight_prior_sigma)).sum()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAvep1Q64Qq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Solver(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        #hidden_size=100\n",
        "        super(Solver, self).__init__()\n",
        "        self.l1 = BayesLinear(input_size, hidden_size)\n",
        "        self.l2 = BayesLinear(hidden_size, hidden_size)\n",
        "        self.l3 = BayesLinear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, sampling=False):\n",
        "        output = self.l1(x, sampling=sampling)\n",
        "        output = self.relu(output)\n",
        "        output = self.l2(output, sampling=sampling)\n",
        "        output = self.relu(output)\n",
        "        return self.l3(output, sampling=sampling)\n",
        "    \n",
        "    \n",
        "    def log_prior(self):\n",
        "        return self.l1.log_prior \\\n",
        "               + self.l2.log_prior \\\n",
        "               + self.l3.log_prior\n",
        "    \n",
        "    def log_variational_posterior(self):\n",
        "        return self.l1.log_variational_posterior \\\n",
        "               + self.l2.log_variational_posterior \\\n",
        "               + self.l2.log_variational_posterior\n",
        "    \n",
        "    \n",
        "    def loss(self, gans, solvers, x_curr, y_curr):\n",
        "        kl = self.l1.kl() + self.l2.kl() + self.l3.kl()\n",
        "        sumloglik = 0.\n",
        "        if len(gans) > 0:\n",
        "            for gan, solver in zip(gans[:-1], solvers[:-1]):\n",
        "                x_gan = gan(sample_noise_batch(batch_size=len(x_curr), code_size=code_size))\n",
        "                y_gan = solver(x_gan, sampling=False)\n",
        "                logits = self.forward(x_gan, sampling=False)\n",
        "                sumloglik += F.binary_cross_entropy_with_logits(logits, y_gan)\n",
        "            x_gan_new = gans[-1](sample_noise_batch(batch_size=60000, code_size=code_size))\n",
        "            y_gan_new = solvers[-1](x_gan_new, sampling=False)\n",
        "            logits = self.forward(torch.cat([x_gan_new, x_curr],dim=0), sampling=False)\n",
        "            sumloglik += F.binary_cross_entropy_with_logits(logits, torch.cat([y_gan_new, y_curr],dim=0)) \n",
        "                \n",
        "        else:\n",
        "            logits = self.forward(x_curr, sampling=False)\n",
        "            sumloglik = F.binary_cross_entropy_with_logits(logits, y_curr)\n",
        "            \n",
        "        return kl + sumloglik #actually negsumloglik"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBNH_XgD4UZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def solver_train_predict(input_size, hidden_size, output_size, batch_size, train_loader, n_epochs, seen_tasks,\n",
        "                         gans, solvers):\n",
        "    net = Solver(input_size, hidden_size, output_size)\n",
        "    net.to(device)\n",
        "    optimizer = optim.Adam(net.parameters())\n",
        "    \n",
        "    #training\n",
        "    net.train()\n",
        "    loss_list = []\n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        cumulative_loss = 0.\n",
        "    \n",
        "        for x_train, y_train in train_loader:\n",
        "            x_train = x_train.to(device)\n",
        "            y_train = y_train.to(device)\n",
        "            \n",
        "            net.zero_grad()\n",
        "            loss = net.loss(gans, solvers, x_train, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            cumulative_loss += loss.detach().cpu().numpy()\n",
        "       \n",
        "        loss_list.append(cumulative_loss)\n",
        "        print('solver_losses = {}\\n'.format(loss_list))\n",
        "    '''\n",
        "    #prediction\n",
        "    preds = []\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test = x_test.to(device)\n",
        "            y_test = y_test.to(device)\n",
        "            logits = net(x_test, sampling=True)\n",
        "            pred_idx = logits.max(1, keepdims=True)[1].transpose(0,1).detach().cpu().numpy()[0]\n",
        "            y_pred = np.zeros(y_test.size())\n",
        "            y_pred[:,0:10] = torch.Tensor(np.eye(10)[pred_idx])\n",
        "        print('pred = {}, test = {}'.format(y_pred,y_test))\n",
        "    '''\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjY6ro8A6cRI",
        "colab_type": "text"
      },
      "source": [
        "# Main loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XkRIHf36bde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scholar_train(n_tasks=5, batch_size=256, gan_epochs=301, solver_epochs=5):\n",
        "    gans = []\n",
        "    solvers = []\n",
        "    data_gen = PermutedMnistGenerator(max_iter=n_tasks, random_seed=0)\n",
        "    print('\\n dataset generated, starting tasks')\n",
        "    for task in tqdm(range(n_tasks)):\n",
        "        #load task data\n",
        "        x_new_train, y_new_train, x_new_test, y_new_test = data_gen.next_task()\n",
        "        print('task {} data loaded'.format(task))\n",
        "        if task !=0:\n",
        "            '''\n",
        "            #cocnatenate with prev GAN data\n",
        "            x_train = torch.cat([torch.Tensor(x_new_train), x_gen], 0).to(device)\n",
        "            y_train = torch.cat([torch.Tensor(y_new_train), y_gen_pred], 0).to(device) \n",
        "            \n",
        "            train = data_utils.TensorDataset(x_train, y_train)\n",
        "            train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "            '''\n",
        "            #train curr task Solver\n",
        "            print('running solver task {}'.format(task))\n",
        "            curr_solver = solver_train_predict(input_size=784, hidden_size=100, output_size=10, batch_size=batch_size, \n",
        "                             train_loader=train_loader, n_epochs=solver_epochs, seen_tasks=task, gans=gans, solvers=solvers)\n",
        "            solvers.append(curr_solver)\n",
        "            \n",
        "            #train curr task GAN\n",
        "            print('running GAN task {}'.format(task))\n",
        "            curr_generator = GAN_train(discr_input=784, discr_output=1, gen_input=code_size, gen_output=784, \n",
        "                                   batch_size=batch_size, data_loader=train_loader, n_epochs=gan_epochs)\n",
        "            gans.append(curr_generator)\n",
        "            \n",
        "            \n",
        "        else:\n",
        "            print('running task {}'.format(task))\n",
        "            x_train = torch.Tensor(x_new_train).to(device)\n",
        "            y_train = torch.Tensor(y_new_train).to(device)\n",
        "            train = data_utils.TensorDataset(x_train, y_train)\n",
        "            train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            #train first Solver\n",
        "            print('running solver task {}'.format(task))\n",
        "            curr_solver = solver_train_predict(input_size=784, hidden_size=100, output_size=10, batch_size=batch_size, \n",
        "                             train_loader=train_loader, n_epochs=solver_epochs, seen_tasks=task, gans=gans, solvers=solvers)\n",
        "            solvers.append(curr_solver)\n",
        "            \n",
        "            #train first GAN\n",
        "            print('running GAN task {}'.format(task))\n",
        "            curr_generator = GAN_train(discr_input=784, discr_output=1, gen_input=code_size, gen_output=784, \n",
        "                                   batch_size=batch_size, data_loader=train_loader, n_epochs=gan_epochs)\n",
        "            gans.append(curr_generator)\n",
        "        print('gans after task {}: {}'.format(task, gans))\n",
        "        print('solvers after task {}: {}'.format(task, solvers))\n",
        "        \n",
        "        ''' \n",
        "        x_gen = gan_generators[0](sample_noise_batch(batch_size=6000, code_size=code_size))\n",
        "        if len(gan_generators) > 1:\n",
        "            for i in range(1, len(gan_generators)):\n",
        "                curr_x_gen =  gan_generators[i](sample_noise_batch(batch_size=6000, code_size=code_size))           \n",
        "                x_gen = torch.cat([x_gen, curr_x_gen], 0)\n",
        "        #train solver\n",
        "        y_gen_pred = solver_train_predict(#x_gen...)\n",
        "        '''\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf1ybrVXAz5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50aeefee-9df1-4a5e-e355-3259f2b13711"
      },
      "source": [
        "scholar_train(n_tasks=2, batch_size=256, gan_epochs=301, solver_epochs=200)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " dataset generated, starting tasks\n",
            "task 0 data loaded\n",
            "running task 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "running solver task 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:01<00:05,  1.50s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver_losses = [43881169.875]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:03<00:04,  1.50s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver_losses = [43881169.875, 42063100.75]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:04<00:03,  1.51s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver_losses = [43881169.875, 42063100.75, 40899046.5]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:05<00:01,  1.49s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver_losses = [43881169.875, 42063100.75, 40899046.5, 39685969.234375]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:07<00:00,  1.49s/it]\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver_losses = [43881169.875, 42063100.75, 40899046.5, 39685969.234375, 38497550.609375]\n",
            "\n",
            "running GAN task 0\n",
            "epoch 0\n",
            "\n",
            "dlosses [1.1353232]\n",
            "\n",
            "glosses [0.7820059]\n",
            "\n",
            "epoch 100\n",
            "\n",
            "dlosses [1.1353232, 1.1051829904317856, 1.0585882402956486, 1.0099667178466916, 0.9639964462351053, 0.9197553610284811, 0.8764357490793566, 0.834478583041123, 0.7940935247124502, 0.7553124450787608, 0.7181558723517973, 0.6826705555716189, 0.648862825663793, 0.6166937859787917, 0.5860956539642221, 0.5569894264449513, 0.5292870027266723, 0.5029275462062737, 0.4778605201870204, 0.4540352267364879, 0.43139533014313836, 0.40989106166090516, 0.3894521193563686, 0.37002896478954167, 0.3515782094738973, 0.3340329222800897, 0.31735982158134696, 0.30152037591754133, 0.28647290253692603, 0.2721778028253415, 0.25859745809933615, 0.24569613060963108, 0.23343986949441128, 0.22179642143495246, 0.21073514577846658, 0.200226933904805, 0.1902441326248265, 0.18076047140884693, 0.17175099325366633, 0.16319198900624476, 0.15506093497119428, 0.1473364336378963, 0.13999815737126325, 0.13302679491796185, 0.1264040005873255, 0.12011234597322099, 0.11413527408982169, 0.10845705580059235, 0.1030627484258245, 0.09793815641979503, 0.09306979401406704, 0.08844484972862544, 0.08405115265745593, 0.07987714043984488, 0.0759118288331144, 0.07214478280672043, 0.06856608908164617, 0.06516633004282563, 0.0619365589559461, 0.058868276423410554, 0.05595340801750178, 0.05318428303188845, 0.050553614295555785, 0.048054478996039755, 0.045680300461499526, 0.04342483085368631, 0.04128213472626375, 0.03924657340521232, 0.037312790150213465, 0.03547569605796455, 0.033730456670328084, 0.03207247925207344, 0.030497400704731528, 0.02900107608475671, 0.027579567695780634, 0.02622913472625336, 0.02494622340520245, 0.02372745765020409, 0.022569630182955644, 0.02146969408906962, 0.0204247547998779, 0.019432062475145763, 0.018489004766650233, 0.01759309994357948, 0.016741990361662264, 0.01593343625884091, 0.015165309861160624, 0.014435589783364353, 0.013742355709457895, 0.013083783339246759, 0.01245813958754618, 0.011863778023430631, 0.011299134537520858, 0.010762723225906575, 0.010253132479873006, 0.009769021271141116, 0.00930911562284582, 0.008872205256965289, 0.008457140409378784, 0.008062828804171605, 0.007688232779224785]\n",
            "\n",
            "glosses [0.7820059, 0.8238801896572112, 0.9416249337792396, 1.0804466402381658, 1.2257750944353638, 1.3842775136485321, 1.5576735203208418, 1.7418026597802636, 1.9331555056501981, 2.128964895360974, 2.325985164081695, 2.5198587974944315, 2.714422808188863, 2.9108552265104986, 3.1062115511835087, 3.2996388095084273, 3.4911835299521954, 3.6815663319091754, 3.868496411462398, 4.0495157894976765, 4.225058641494655, 4.391585145531373, 4.556642099650068, 4.718238371819053, 4.876099508304699, 5.034774467581847, 5.1911826024180865, 5.287258638465395, 5.323074329741587, 5.289681662525138, 5.114785875812302, 4.863825267442438, 4.621722981675421, 4.391131203434002, 4.17185205310454, 3.9634850259038457, 3.765501417573679, 3.577411274366211, 3.3987185938999365, 3.2289412663206374, 3.067672189761504, 2.9144471158926946, 2.7688949331284065, 2.630623401434692, 2.4992264234441257, 2.374405673901082, 2.2558565557817665, 2.1432148415298182, 2.0361786689001073, 1.9345138637236865, 1.8379275727398325, 1.7461628220783383, 1.658990542963532, 1.5761635138377479, 1.4974771534622395, 1.4227245933832873, 1.3517175596920565, 1.2842474451500445, 1.2201539041116938, 1.1592871315274673, 1.1014524298265846, 1.046495476177584, 0.9942790712730346, 0.9446989558646897, 0.8975990311833982, 0.8528272348133104, 0.8102848988557738, 0.7698736204362486, 0.7314910214244073, 0.6950195692755959, 0.6603836960887435, 0.6274496429220583, 0.5961772941693808, 0.5664683980684492, 0.5382288378838971, 0.5114072059052187, 0.48592806855136755, 0.46172085289255277, 0.4387402799450802, 0.4169024584842622, 0.3961602302201345, 0.37644418011949005, 0.35771364553100565, 0.3399371675784177, 0.32303623532572917, 0.3069674464384136, 0.2917036334681644, 0.2772066036475893, 0.263434461773501, 0.25034311752736604, 0.23789886019210923, 0.2260807488269047, 0.21486527218795584, 0.20419290750025262, 0.19406163901423595, 0.18443867051405505, 0.17528865575483224, 0.1665922661045255, 0.15833353754160762, 0.1504785253017054, 0.14301838566900152]\n",
            "\n",
            "epoch 200\n",
            "\n",
            "dlosses [1.1353232, 1.1051829904317856, 1.0585882402956486, 1.0099667178466916, 0.9639964462351053, 0.9197553610284811, 0.8764357490793566, 0.834478583041123, 0.7940935247124502, 0.7553124450787608, 0.7181558723517973, 0.6826705555716189, 0.648862825663793, 0.6166937859787917, 0.5860956539642221, 0.5569894264449513, 0.5292870027266723, 0.5029275462062737, 0.4778605201870204, 0.4540352267364879, 0.43139533014313836, 0.40989106166090516, 0.3894521193563686, 0.37002896478954167, 0.3515782094738973, 0.3340329222800897, 0.31735982158134696, 0.30152037591754133, 0.28647290253692603, 0.2721778028253415, 0.25859745809933615, 0.24569613060963108, 0.23343986949441128, 0.22179642143495246, 0.21073514577846658, 0.200226933904805, 0.1902441326248265, 0.18076047140884693, 0.17175099325366633, 0.16319198900624476, 0.15506093497119428, 0.1473364336378963, 0.13999815737126325, 0.13302679491796185, 0.1264040005873255, 0.12011234597322099, 0.11413527408982169, 0.10845705580059235, 0.1030627484258245, 0.09793815641979503, 0.09306979401406704, 0.08844484972862544, 0.08405115265745593, 0.07987714043984488, 0.0759118288331144, 0.07214478280672043, 0.06856608908164617, 0.06516633004282563, 0.0619365589559461, 0.058868276423410554, 0.05595340801750178, 0.05318428303188845, 0.050553614295555785, 0.048054478996039755, 0.045680300461499526, 0.04342483085368631, 0.04128213472626375, 0.03924657340521232, 0.037312790150213465, 0.03547569605796455, 0.033730456670328084, 0.03207247925207344, 0.030497400704731528, 0.02900107608475671, 0.027579567695780634, 0.02622913472625336, 0.02494622340520245, 0.02372745765020409, 0.022569630182955644, 0.02146969408906962, 0.0204247547998779, 0.019432062475145763, 0.018489004766650233, 0.01759309994357948, 0.016741990361662264, 0.01593343625884091, 0.015165309861160624, 0.014435589783364353, 0.013742355709457895, 0.013083783339246759, 0.01245813958754618, 0.011863778023430631, 0.011299134537520858, 0.010762723225906575, 0.010253132479873006, 0.009769021271141116, 0.00930911562284582, 0.008872205256965289, 0.008457140409378784, 0.008062828804171605, 0.007688232779224785, 0.007332366555525306, 0.006994293643010801, 0.0066731243761220214, 0.00636801357257768, 0.006078158309210557, 0.0058027958090117885, 0.005541201433822959, 0.005292686777393572, 0.005056597853785654, 0.004832313376358131, 0.004619243122801984, 0.004416826381923645, 0.004224530478089223, 0.004041849369446522, 0.0038683023162359564, 0.003703432615685919, 0.0035468064001633833, 0.003398011495416974, 0.003256656335907886, 0.003122368934374252, 0.0029947959029172997, 0.002873601523033195, 0.0027584668621432953, 0.0026490889342978908, 0.0025451799028447563, 0.04863873012897022, 0.10338067753746923, 0.11511766370587621, 0.15505762779345533, 0.18133495439538258, 0.20413444717701723, 0.22527657912152635, 0.24845240928410237, 0.27099474069272284, 0.29003512170866197, 0.30083931626486093, 0.303541882679134, 0.3009494286308744, 0.2957556853148893, 0.2908353464286405, 0.28592690116731667, 0.2813130482757084, 0.26969831166046787, 0.2600996632477545, 0.26176841975707393, 0.299139943971094, 0.31202033648010463, 0.31873666658660327, 0.30756614698894347, 0.3206456064108128, 0.3463940422710735, 0.35758170693398617, 0.35603494543648806, 0.3469520761954406, 0.34333508478286245, 0.3452374836141691, 0.33540959758739514, 0.32410026886087456, 0.3113223123860429, 0.3001119499799079, 0.288343817877436, 0.27750316867558267, 0.26658597772431075, 0.25567700646426395, 0.2446095653921446, 0.23409549637363122, 0.22410713080604353, 0.2146181835168352, 0.2056036835920873, 0.1970399086635768, 0.1889043224814918, 0.18117551560851108, 0.1738331490791794, 0.1668579008763143, 0.16023141508359245, 0.18278839038616027, 0.17517635451098604, 0.16794492042957052, 0.16107505805222577, 0.15454868879374828, 0.14834863799819464, 0.1424585897424187, 0.13686304389943155, 0.13154727534859376, 0.15433855355370588, 0.14939428307671465, 0.144697226123573, 0.14023502201808843, 0.13599592811787808, 0.13196878891267827, 0.12814300666773842, 0.12450851353504558, 0.12105574505898738, 0.11777561500673209, 0.11465949145708956, 0.11169917408492916, 0.10888687258137678, 0.10621518615300202, 0.10367708404604599, 0.12638405511535014]\n",
            "\n",
            "glosses [0.7820059, 0.8238801896572112, 0.9416249337792396, 1.0804466402381658, 1.2257750944353638, 1.3842775136485321, 1.5576735203208418, 1.7418026597802636, 1.9331555056501981, 2.128964895360974, 2.325985164081695, 2.5198587974944315, 2.714422808188863, 2.9108552265104986, 3.1062115511835087, 3.2996388095084273, 3.4911835299521954, 3.6815663319091754, 3.868496411462398, 4.0495157894976765, 4.225058641494655, 4.391585145531373, 4.556642099650068, 4.718238371819053, 4.876099508304699, 5.034774467581847, 5.1911826024180865, 5.287258638465395, 5.323074329741587, 5.289681662525138, 5.114785875812302, 4.863825267442438, 4.621722981675421, 4.391131203434002, 4.17185205310454, 3.9634850259038457, 3.765501417573679, 3.577411274366211, 3.3987185938999365, 3.2289412663206374, 3.067672189761504, 2.9144471158926946, 2.7688949331284065, 2.630623401434692, 2.4992264234441257, 2.374405673901082, 2.2558565557817665, 2.1432148415298182, 2.0361786689001073, 1.9345138637236865, 1.8379275727398325, 1.7461628220783383, 1.658990542963532, 1.5761635138377479, 1.4974771534622395, 1.4227245933832873, 1.3517175596920565, 1.2842474451500445, 1.2201539041116938, 1.1592871315274673, 1.1014524298265846, 1.046495476177584, 0.9942790712730346, 0.9446989558646897, 0.8975990311833982, 0.8528272348133104, 0.8102848988557738, 0.7698736204362486, 0.7314910214244073, 0.6950195692755959, 0.6603836960887435, 0.6274496429220583, 0.5961772941693808, 0.5664683980684492, 0.5382288378838971, 0.5114072059052187, 0.48592806855136755, 0.46172085289255277, 0.4387402799450802, 0.4169024584842622, 0.3961602302201345, 0.37644418011949005, 0.35771364553100565, 0.3399371675784177, 0.32303623532572917, 0.3069674464384136, 0.2917036334681644, 0.2772066036475893, 0.263434461773501, 0.25034311752736604, 0.23789886019210923, 0.2260807488269047, 0.21486527218795584, 0.20419290750025262, 0.19406163901423595, 0.18443867051405505, 0.17528865575483224, 0.1665922661045255, 0.15833353754160762, 0.1504785253017054, 0.14301838566900152, 0.1359472105375777, 0.1292261682286059, 0.12282343225385846, 0.11674260058839665, 0.11096634120291478, 0.10550534495900651, 0.10028704599117041, 0.09533347892744899, 0.09062422690841687, 0.08615672046600072, 0.0819116890187102, 0.0778766546446617, 0.07403596299227165, 0.07039776594394104, 0.06694118398443209, 0.06365308806305334, 0.06052463456155056, 0.05755317217949787, 0.054737171915453006, 0.052050452688657746, 0.049510582603465594, 0.04708779089464846, 0.04478194933017521, 0.04259277737187688, 0.040509317271671966, 0.44957284257324953, 1.168960884404548, 1.3736517066088811, 1.5089350712906442, 1.6622521603088147, 1.8175087787490625, 1.9540536698592166, 2.0718422573852378, 2.1749398360595547, 2.2864734915344087, 2.4047886767065285, 2.531052977459825, 2.654088913486736, 2.7842887065132658, 2.9043404331062423, 3.0182594683509545, 3.1803161795342367, 3.3664708207680962, 3.557869846334306, 3.6762214998153198, 3.601634458180267, 3.5886409696958443, 3.6019458210638047, 3.63574337406579, 3.622993189806165, 3.552985211986999, 3.526117201387649, 3.5365653057424, 3.6263029832195617, 3.7497095709665422, 3.7454137706103534, 3.7422346858357867, 3.7668088951475127, 3.795443900905884, 3.835014118809198, 3.8746658100244997, 3.9198806978145955, 3.9690893234844733, 4.018055072916939, 4.080342922527317, 4.0112324140695605, 3.819682298910268, 3.630415749173428, 3.449865131150943, 3.2781079405473657, 3.114769976152667, 2.9595809771752646, 2.8120995264244053, 2.6720012507716686, 2.5389000893544034, 2.6753797436620124, 2.810981240831023, 2.796732728296613, 2.6808607144985497, 2.5519610347243353, 2.4265752559450857, 2.3067849186455422, 2.192669058326677, 2.084119582930809, 2.279751026155179, 2.503222419168099, 2.5875682867350847, 2.6329268371009364, 2.6226680246190583, 2.5369266330040032, 2.424025003204217, 2.310536083713171, 2.200860812791434, 2.0957949008065806, 1.9950061413886573, 1.8988983680387639, 1.8074537361265508, 1.7202106452108643, 1.6371714132468065, 1.778134936548089]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 50%|█████     | 1/2 [00:23<00:23, 23.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 300\n",
            "\n",
            "dlosses [1.1353232, 1.1051829904317856, 1.0585882402956486, 1.0099667178466916, 0.9639964462351053, 0.9197553610284811, 0.8764357490793566, 0.834478583041123, 0.7940935247124502, 0.7553124450787608, 0.7181558723517973, 0.6826705555716189, 0.648862825663793, 0.6166937859787917, 0.5860956539642221, 0.5569894264449513, 0.5292870027266723, 0.5029275462062737, 0.4778605201870204, 0.4540352267364879, 0.43139533014313836, 0.40989106166090516, 0.3894521193563686, 0.37002896478954167, 0.3515782094738973, 0.3340329222800897, 0.31735982158134696, 0.30152037591754133, 0.28647290253692603, 0.2721778028253415, 0.25859745809933615, 0.24569613060963108, 0.23343986949441128, 0.22179642143495246, 0.21073514577846658, 0.200226933904805, 0.1902441326248265, 0.18076047140884693, 0.17175099325366633, 0.16319198900624476, 0.15506093497119428, 0.1473364336378963, 0.13999815737126325, 0.13302679491796185, 0.1264040005873255, 0.12011234597322099, 0.11413527408982169, 0.10845705580059235, 0.1030627484258245, 0.09793815641979503, 0.09306979401406704, 0.08844484972862544, 0.08405115265745593, 0.07987714043984488, 0.0759118288331144, 0.07214478280672043, 0.06856608908164617, 0.06516633004282563, 0.0619365589559461, 0.058868276423410554, 0.05595340801750178, 0.05318428303188845, 0.050553614295555785, 0.048054478996039755, 0.045680300461499526, 0.04342483085368631, 0.04128213472626375, 0.03924657340521232, 0.037312790150213465, 0.03547569605796455, 0.033730456670328084, 0.03207247925207344, 0.030497400704731528, 0.02900107608475671, 0.027579567695780634, 0.02622913472625336, 0.02494622340520245, 0.02372745765020409, 0.022569630182955644, 0.02146969408906962, 0.0204247547998779, 0.019432062475145763, 0.018489004766650233, 0.01759309994357948, 0.016741990361662264, 0.01593343625884091, 0.015165309861160624, 0.014435589783364353, 0.013742355709457895, 0.013083783339246759, 0.01245813958754618, 0.011863778023430631, 0.011299134537520858, 0.010762723225906575, 0.010253132479873006, 0.009769021271141116, 0.00930911562284582, 0.008872205256965289, 0.008457140409378784, 0.008062828804171605, 0.007688232779224785, 0.007332366555525306, 0.006994293643010801, 0.0066731243761220214, 0.00636801357257768, 0.006078158309210557, 0.0058027958090117885, 0.005541201433822959, 0.005292686777393572, 0.005056597853785654, 0.004832313376358131, 0.004619243122801984, 0.004416826381923645, 0.004224530478089223, 0.004041849369446522, 0.0038683023162359564, 0.003703432615685919, 0.0035468064001633833, 0.003398011495416974, 0.003256656335907886, 0.003122368934374252, 0.0029947959029172997, 0.002873601523033195, 0.0027584668621432953, 0.0026490889342978908, 0.0025451799028447563, 0.04863873012897022, 0.10338067753746923, 0.11511766370587621, 0.15505762779345533, 0.18133495439538258, 0.20413444717701723, 0.22527657912152635, 0.24845240928410237, 0.27099474069272284, 0.29003512170866197, 0.30083931626486093, 0.303541882679134, 0.3009494286308744, 0.2957556853148893, 0.2908353464286405, 0.28592690116731667, 0.2813130482757084, 0.26969831166046787, 0.2600996632477545, 0.26176841975707393, 0.299139943971094, 0.31202033648010463, 0.31873666658660327, 0.30756614698894347, 0.3206456064108128, 0.3463940422710735, 0.35758170693398617, 0.35603494543648806, 0.3469520761954406, 0.34333508478286245, 0.3452374836141691, 0.33540959758739514, 0.32410026886087456, 0.3113223123860429, 0.3001119499799079, 0.288343817877436, 0.27750316867558267, 0.26658597772431075, 0.25567700646426395, 0.2446095653921446, 0.23409549637363122, 0.22410713080604353, 0.2146181835168352, 0.2056036835920873, 0.1970399086635768, 0.1889043224814918, 0.18117551560851108, 0.1738331490791794, 0.1668579008763143, 0.16023141508359245, 0.18278839038616027, 0.17517635451098604, 0.16794492042957052, 0.16107505805222577, 0.15454868879374828, 0.14834863799819464, 0.1424585897424187, 0.13686304389943155, 0.13154727534859376, 0.15433855355370588, 0.14939428307671465, 0.144697226123573, 0.14023502201808843, 0.13599592811787808, 0.13196878891267827, 0.12814300666773842, 0.12450851353504558, 0.12105574505898738, 0.11777561500673209, 0.11465949145708956, 0.11169917408492916, 0.10888687258137678, 0.10621518615300202, 0.10367708404604599, 0.12638405511535014, 0.12175465521483585, 0.11735672530934727, 0.11317869189913311, 0.10920956015942967, 0.1054388850067114, 0.10185674361162905, 0.09845370928630082, 0.095220826677239, 0.09214958819863026, 0.08923191164395196, 0.08646011891700757, 0.0838269158264104, 0.0813253728903431, 0.07894890710107916, 0.1057584151046754, 0.10426189364954004, 0.10284019826716145, 0.1014895876539018, 0.10020650757130513, 0.09898758149283829, 0.09782960171829479, 0.09672952093247847, 0.11589646714613897, 0.11372269264163333, 0.1111089485286342, 0.10862589162128504, 0.10626698755930333, 0.10402602870042071, 0.10189711778448222, 0.09987465241434065, 0.09795331031270617, 0.09612803531615341, 0.09439402406942829, 0.11181456800555573, 0.10924895470035688, 0.10681162206041797, 0.104496156052476, 0.10229646334493114, 0.10020675527276351, 0.09822153260420427, 0.09633557106907299, 0.09454390761069828, 0.09284182732524231, 0.11407341488163339, 0.11141612169953816, 0.10889169317654769, 0.10649348607970675, 0.10421518933770786, 0.10205080743280891, 0.11925209118788889, 0.11607299708994406, 0.11305285769689648, 0.11018372527350126, 0.10745804947127581, 0.10486865745916163, 0.10240873504765316, 0.10007180875672012, 0.13445147773451063, 0.12963683697864542, 0.12506292826057347, 0.12071771497840511, 0.11658976236034517, 0.11266820737318822, 0.10894273013538913, 0.10540352675947999, 0.1270587115779696, 0.12507100374305438, 0.12270560833080592, 0.12045848268916988, 0.11832371332961564, 0.1162956824380391, 0.1143690530910414, 0.14200223934964393, 0.13958013944030057, 0.1363149924585247, 0.13283873089488074, 0.129536282409419, 0.12639895634823034, 0.12341849659010112, 0.12058705981987837, 0.11789719488816675, 0.14623979146547472, 0.14265764971081402, 0.13777915440280028, 0.1331445838601872, 0.1287417418447048, 0.1245590419299965, 0.12058547701102362, 0.11681059033799938, 0.11322444799862635, 0.10981761277622198, 0.10658111931493783, 0.10350645052671788, 0.12464870953476766, 0.12122208224456033, 0.11796678631886337, 0.11487425518945125, 0.11193635061650975, 0.10914534127221531, 0.1064938823951356]\n",
            "\n",
            "glosses [0.7820059, 0.8238801896572112, 0.9416249337792396, 1.0804466402381658, 1.2257750944353638, 1.3842775136485321, 1.5576735203208418, 1.7418026597802636, 1.9331555056501981, 2.128964895360974, 2.325985164081695, 2.5198587974944315, 2.714422808188863, 2.9108552265104986, 3.1062115511835087, 3.2996388095084273, 3.4911835299521954, 3.6815663319091754, 3.868496411462398, 4.0495157894976765, 4.225058641494655, 4.391585145531373, 4.556642099650068, 4.718238371819053, 4.876099508304699, 5.034774467581847, 5.1911826024180865, 5.287258638465395, 5.323074329741587, 5.289681662525138, 5.114785875812302, 4.863825267442438, 4.621722981675421, 4.391131203434002, 4.17185205310454, 3.9634850259038457, 3.765501417573679, 3.577411274366211, 3.3987185938999365, 3.2289412663206374, 3.067672189761504, 2.9144471158926946, 2.7688949331284065, 2.630623401434692, 2.4992264234441257, 2.374405673901082, 2.2558565557817665, 2.1432148415298182, 2.0361786689001073, 1.9345138637236865, 1.8379275727398325, 1.7461628220783383, 1.658990542963532, 1.5761635138377479, 1.4974771534622395, 1.4227245933832873, 1.3517175596920565, 1.2842474451500445, 1.2201539041116938, 1.1592871315274673, 1.1014524298265846, 1.046495476177584, 0.9942790712730346, 0.9446989558646897, 0.8975990311833982, 0.8528272348133104, 0.8102848988557738, 0.7698736204362486, 0.7314910214244073, 0.6950195692755959, 0.6603836960887435, 0.6274496429220583, 0.5961772941693808, 0.5664683980684492, 0.5382288378838971, 0.5114072059052187, 0.48592806855136755, 0.46172085289255277, 0.4387402799450802, 0.4169024584842622, 0.3961602302201345, 0.37644418011949005, 0.35771364553100565, 0.3399371675784177, 0.32303623532572917, 0.3069674464384136, 0.2917036334681644, 0.2772066036475893, 0.263434461773501, 0.25034311752736604, 0.23789886019210923, 0.2260807488269047, 0.21486527218795584, 0.20419290750025262, 0.19406163901423595, 0.18443867051405505, 0.17528865575483224, 0.1665922661045255, 0.15833353754160762, 0.1504785253017054, 0.14301838566900152, 0.1359472105375777, 0.1292261682286059, 0.12282343225385846, 0.11674260058839665, 0.11096634120291478, 0.10550534495900651, 0.10028704599117041, 0.09533347892744899, 0.09062422690841687, 0.08615672046600072, 0.0819116890187102, 0.0778766546446617, 0.07403596299227165, 0.07039776594394104, 0.06694118398443209, 0.06365308806305334, 0.06052463456155056, 0.05755317217949787, 0.054737171915453006, 0.052050452688657746, 0.049510582603465594, 0.04708779089464846, 0.04478194933017521, 0.04259277737187688, 0.040509317271671966, 0.44957284257324953, 1.168960884404548, 1.3736517066088811, 1.5089350712906442, 1.6622521603088147, 1.8175087787490625, 1.9540536698592166, 2.0718422573852378, 2.1749398360595547, 2.2864734915344087, 2.4047886767065285, 2.531052977459825, 2.654088913486736, 2.7842887065132658, 2.9043404331062423, 3.0182594683509545, 3.1803161795342367, 3.3664708207680962, 3.557869846334306, 3.6762214998153198, 3.601634458180267, 3.5886409696958443, 3.6019458210638047, 3.63574337406579, 3.622993189806165, 3.552985211986999, 3.526117201387649, 3.5365653057424, 3.6263029832195617, 3.7497095709665422, 3.7454137706103534, 3.7422346858357867, 3.7668088951475127, 3.795443900905884, 3.835014118809198, 3.8746658100244997, 3.9198806978145955, 3.9690893234844733, 4.018055072916939, 4.080342922527317, 4.0112324140695605, 3.819682298910268, 3.630415749173428, 3.449865131150943, 3.2781079405473657, 3.114769976152667, 2.9595809771752646, 2.8120995264244053, 2.6720012507716686, 2.5389000893544034, 2.6753797436620124, 2.810981240831023, 2.796732728296613, 2.6808607144985497, 2.5519610347243353, 2.4265752559450857, 2.3067849186455422, 2.192669058326677, 2.084119582930809, 2.279751026155179, 2.503222419168099, 2.5875682867350847, 2.6329268371009364, 2.6226680246190583, 2.5369266330040032, 2.424025003204217, 2.310536083713171, 2.200860812791434, 2.0957949008065806, 1.9950061413886573, 1.8988983680387639, 1.8074537361265508, 1.7202106452108643, 1.6371714132468065, 1.778134936548089, 1.9506557352681697, 2.0618131865037235, 2.0901244236552827, 2.028802652741264, 1.9386835596020529, 1.8478790487848453, 1.7600229812366361, 1.6756225711174273, 1.5949725080792199, 1.5181447159052774, 1.4450838628546239, 1.375335271281903, 1.308837944509347, 1.2455154360877194, 1.490954364982186, 1.6953511390731644, 1.808909883694213, 1.8098931939070853, 1.744097003254509, 1.6638513634279917, 1.5846079747569335, 1.5080461545693065, 1.6477358621072595, 1.8077549127366377, 1.9673081346596202, 2.008744492414859, 1.9529769257397491, 1.8679377043154324, 1.7819513319642504, 1.6974607298051916, 1.6162299528531057, 1.538312061225, 1.463816447347295, 1.6819201520337632, 1.8807240009994577, 1.9726311422046727, 1.942086882772433, 1.859561813884246, 1.7710209146018865, 1.684962205614372, 1.6025224175546942, 1.524085067999329, 1.4492543036141667, 1.6130287329036732, 1.8020256574645441, 1.8418136243929526, 1.7885171090229905, 1.7082034358314007, 1.6269011524552863, 1.7867226149512354, 1.9937845039943232, 2.022853327805837, 1.9329623874170463, 1.8388319727118547, 1.7482274609512534, 1.6617662530897679, 1.5794073241497764, 1.8986350618470116, 2.086120391541221, 2.090881549736682, 2.0115585667389455, 1.9212661068015227, 1.8316378940000417, 1.7450316106440193, 1.6621051659090806, 1.773675901532999, 1.9212116079517103, 2.0593441141782702, 2.0613764983894005, 1.9694136571742589, 1.8765750656291464, 1.7865319128609087, 1.9117644908018476, 2.0386635728566036, 2.1735651586943643, 2.302420350886294, 2.277661631830596, 2.171236122460822, 2.0666801366419363, 1.9661866105770298, 1.8701665622061954, 2.0007338852647085, 2.1391031816020587, 2.2831747006072827, 2.347513136978896, 2.288585073580497, 2.187744302042967, 2.087574128671699, 1.9904991906344915, 1.8972374067094944, 1.807733668348955, 1.7218764186289695, 1.6398266864187963, 1.8705227491010301, 2.071473073056501, 2.20165181312621, 2.2160408034521075, 2.1333649434514688, 2.0446098798488035, 1.9543145573413103]\n",
            "\n",
            "gans after task 0: [GAN_generator(\n",
            "  (generator_net): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=256, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.2)\n",
            "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
            "  )\n",
            ")]\n",
            "solvers after task 0: [Solver(\n",
            "  (l1): BayesLinear()\n",
            "  (l2): BayesLinear()\n",
            "  (l3): BayesLinear()\n",
            "  (relu): ReLU()\n",
            ")]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "task 1 data loaded\n",
            "running solver task 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [01:34<06:17, 94.45s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver_losses = [43881169.875]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 40%|████      | 2/5 [03:08<04:43, 94.47s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver_losses = [43881169.875, 42063100.75]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 60%|██████    | 3/5 [04:43<03:08, 94.45s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver_losses = [43881169.875, 42063100.75, 40899046.5]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 80%|████████  | 4/5 [06:17<01:34, 94.47s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver_losses = [43881169.875, 42063100.75, 40899046.5, 39685969.234375]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 5/5 [07:52<00:00, 94.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver_losses = [43881169.875, 42063100.75, 40899046.5, 39685969.234375, 38497550.609375]\n",
            "\n",
            "running GAN task 1\n",
            "epoch 0\n",
            "\n",
            "dlosses [1.1277566]\n",
            "\n",
            "glosses [0.79874367]\n",
            "\n",
            "epoch 100\n",
            "\n",
            "dlosses [1.1277566, 1.0990544736385346, 1.0556731632351877, 1.0083886109739544, 0.9631030070833863, 0.9191359882702306, 0.8757713441940527, 0.8336708319333952, 0.7932278417153034, 0.7544894801736105, 0.717388494032345, 0.6819395334403571, 0.6481406121461281, 0.6159681260954095, 0.5853667614306972, 0.556267888095408, 0.5285869388900527, 0.5022557897829103, 0.47721963418345853, 0.453422160297792, 0.4308079320349851, 0.4093202472897873, 0.38891048840951536, 0.36950912812043507, 0.35107043489888434, 0.3335483703608729, 0.31693213339716725, 0.30114670828164686, 0.2861505544219025, 0.2719042082551454, 0.2583701793967261, 0.24551285198122771, 0.23329839093650429, 0.22169465294401702, 0.21067110185115412, 0.20019872831293437, 0.1902499734516256, 0.18079865633338227, 0.1718199050710511, 0.1632900913718365, 0.15518676835758263, 0.14748861149404147, 0.14017536247367735, 0.13322777590433144, 0.12662756866345282, 0.12035737178461813, 0.11440068474972517, 0.10874183206657687, 0.10336592201758597, 0.09825880747104462, 0.09340704865183035, 0.08879787777357678, 0.0844191654392359, 0.08025938872161206, 0.07630760083986941, 0.0725534023522139, 0.06898691378894116, 0.06559874965383206, 0.062379993725478415, 0.059322175593542456, 0.0564172483682033, 0.053657567504131094, 0.0510358706832625, 0.04854525870343734, 0.04617917732260343, 0.04393140001081122, 0.041796011564608625, 0.03976739254071616, 0.037840204468018314, 0.03600937579895536, 0.03427008856334556, 0.03261776568951625, 0.0310480589593784, 0.029556837565747444, 0.028140177241798035, 0.026794349934046096, 0.025515813991681756, 0.024301204846435633, 0.023147326158451817, 0.02205114140486719, 0.021009765888961796, 0.02002045914885167, 0.019080617745747052, 0.018187768412797664, 0.017339561546495744, 0.016533765023508923, 0.01576825832667144, 0.015041026964675831, 0.014350157170780002, 0.013693830866578964, 0.013070320877587978, 0.012477986388046541, 0.011915268622982176, 0.01138068674617103, 0.010872833963200441, 0.010390373819378382, 0.009932036682747426, 0.09914883545095075, 0.1455318365742925, 0.17567980314668566, 0.20425326252090653]\n",
            "\n",
            "glosses [0.79874367, 0.8278134077787399, 0.9248773415386676, 1.0504994860813022, 1.1895595757115633, 1.3415605455191248, 1.515767603921269, 1.7037651820153057, 1.8986448912830707, 2.097694341894088, 2.2983747674309143, 2.499358381392437, 2.7000648054319463, 2.9025901277061252, 3.10150106283388, 3.302110170878404, 3.5030307062263906, 3.70198886825699, 3.895633860154504, 4.081900369661428, 4.261233400265881, 4.435517592641197, 4.600864820919294, 4.76733184713102, 4.932188483229059, 5.096837184372782, 5.248400201863127, 5.305548587384595, 5.169313282763595, 4.912474152826619, 4.667122616105213, 4.433885481526182, 4.212261993076176, 4.001696038501955, 3.801653603040316, 3.611617436691254, 3.4310759586840107, 3.2595577018004116, 3.096616452033004, 2.9418267821021002, 2.794778628384035, 2.655078642406899, 2.522363194624992, 2.3962823116414955, 2.276505200377858, 2.162717087900263, 2.0546174978035006, 1.951924017610172, 1.854368245623071, 1.761685786773424, 1.6736397046959937, 1.589994711119465, 1.5105355057265881, 1.435045265482713, 1.3633334333226075, 1.2952017275600918, 1.230480546874141, 1.1689940770719431, 1.1105844423185602, 1.0550926886078398, 1.0023787143619067, 0.9522936240441326, 0.9047182900856541, 0.8595226569823872, 0.8165774417260802, 0.7757833228147628, 0.737028874272489, 0.7002077023609621, 0.6652362665965343, 0.6320098072495287, 0.6004439690950303, 0.5704585569681158, 0.5419701421357085, 0.514902074568037, 0.489190169707094, 0.46475962708548774, 0.44155163059282565, 0.41950674401237076, 0.3985655725259211, 0.37867069275114446, 0.35976921386407645, 0.3418157049591295, 0.324755079111367, 0.3085497724051944, 0.2931574626209041, 0.27853349125265825, 0.2646467929606619, 0.2514458712148386, 0.23890602982203976, 0.22699348867061042, 0.21567478673426838, 0.20492602425876177, 0.19471159120466333, 0.18500814131573837, 0.1757867981674151, 0.167027498543081, 0.15870116595445652, 0.8630562039396927, 1.2670380792590654, 1.3669087028000795, 1.475746585798198]\n",
            "\n",
            "epoch 200\n",
            "\n",
            "dlosses [1.1277566, 1.0990544736385346, 1.0556731632351877, 1.0083886109739544, 0.9631030070833863, 0.9191359882702306, 0.8757713441940527, 0.8336708319333952, 0.7932278417153034, 0.7544894801736105, 0.717388494032345, 0.6819395334403571, 0.6481406121461281, 0.6159681260954095, 0.5853667614306972, 0.556267888095408, 0.5285869388900527, 0.5022557897829103, 0.47721963418345853, 0.453422160297792, 0.4308079320349851, 0.4093202472897873, 0.38891048840951536, 0.36950912812043507, 0.35107043489888434, 0.3335483703608729, 0.31693213339716725, 0.30114670828164686, 0.2861505544219025, 0.2719042082551454, 0.2583701793967261, 0.24551285198122771, 0.23329839093650429, 0.22169465294401702, 0.21067110185115412, 0.20019872831293437, 0.1902499734516256, 0.18079865633338227, 0.1718199050710511, 0.1632900913718365, 0.15518676835758263, 0.14748861149404147, 0.14017536247367735, 0.13322777590433144, 0.12662756866345282, 0.12035737178461813, 0.11440068474972517, 0.10874183206657687, 0.10336592201758597, 0.09825880747104462, 0.09340704865183035, 0.08879787777357678, 0.0844191654392359, 0.08025938872161206, 0.07630760083986941, 0.0725534023522139, 0.06898691378894116, 0.06559874965383206, 0.062379993725478415, 0.059322175593542456, 0.0564172483682033, 0.053657567504131094, 0.0510358706832625, 0.04854525870343734, 0.04617917732260343, 0.04393140001081122, 0.041796011564608625, 0.03976739254071616, 0.037840204468018314, 0.03600937579895536, 0.03427008856334556, 0.03261776568951625, 0.0310480589593784, 0.029556837565747444, 0.028140177241798035, 0.026794349934046096, 0.025515813991681756, 0.024301204846435633, 0.023147326158451817, 0.02205114140486719, 0.021009765888961796, 0.02002045914885167, 0.019080617745747052, 0.018187768412797664, 0.017339561546495744, 0.016533765023508923, 0.01576825832667144, 0.015041026964675831, 0.014350157170780002, 0.013693830866578964, 0.013070320877587978, 0.012477986388046541, 0.011915268622982176, 0.01138068674617103, 0.010872833963200441, 0.010390373819378382, 0.009932036682747426, 0.09914883545095075, 0.1455318365742925, 0.17567980314668566, 0.20425326252090653, 0.22477866106134986, 0.24242073246098775, 0.2589078391675587, 0.2725645246933116, 0.2839980369497044, 0.290093108356423, 0.2896010668311571, 0.28560261916839597, 0.2800551736567154, 0.27634067361583786, 0.2743480510888393, 0.2649338654686014, 0.2530516898727467, 0.2467207397313165, 0.23691356893893928, 0.2525694080262483, 0.287246855364804, 0.28596946512980886, 0.2866816080565993, 0.30712041845996746, 0.3469952653996705, 0.3721741075908076, 0.41116073864849256, 0.42512007039034616, 0.42261202541174525, 0.41693966822646983, 0.4139972063400525, 0.41393826463553, 0.4168033506055281, 0.4165398672521735, 0.4106556435958141, 0.4002182513484174, 0.3854334241160127, 0.3694228332944665, 0.37209531432614285, 0.36243728551183424, 0.3564890045947688, 0.3496098648086701, 0.34002344648507615, 0.32775208302780007, 0.31546218421083505, 0.30320531504572157, 0.29167990444461195, 0.28230033460201526, 0.27224424926732593, 0.2626909681993711, 0.25361535118481404, 0.24499351502098482, 0.2924271264136429, 0.2798658276883329, 0.26810085650763776, 0.2571438775848128, 0.24673474760812902, 0.23684607413027944, 0.22745183432632235, 0.2185273065125631, 0.23967541647532606, 0.23115179437349312, 0.22305435337675183, 0.2153617844298476, 0.20805384393028858, 0.20111130045570752, 0.19451588415485552, 0.22139509342325564, 0.21553475080532028, 0.21036905259064914, 0.20282287199957647, 0.19565400043805742, 0.18884357245461433, 0.1823736658703434, 0.17622725461528602, 0.20344975769396234, 0.19741929566742805, 0.19179753222652013, 0.1864568569576576, 0.18138321545223818, 0.17656325602208975, 0.19449791549137863, 0.187572091088365, 0.1813062362893293, 0.17535367423024537, 0.16969874027411563, 0.16432655301579238, 0.15922297512038527, 0.15437457611974853, 0.14976859706914364, 0.16653828012309613, 0.18246947902435098, 0.18715303658539384, 0.17927860936785353, 0.17179790351119023, 0.1646912329473601, 0.15793989591172147, 0.15152612572786478, 0.14543304405320093, 0.13964461646227028, 0.13414561025088614, 0.12892155435007122, 0.12395870124429703, 0.11924399079381155]\n",
            "\n",
            "glosses [0.79874367, 0.8278134077787399, 0.9248773415386676, 1.0504994860813022, 1.1895595757115633, 1.3415605455191248, 1.515767603921269, 1.7037651820153057, 1.8986448912830707, 2.097694341894088, 2.2983747674309143, 2.499358381392437, 2.7000648054319463, 2.9025901277061252, 3.10150106283388, 3.302110170878404, 3.5030307062263906, 3.70198886825699, 3.895633860154504, 4.081900369661428, 4.261233400265881, 4.435517592641197, 4.600864820919294, 4.76733184713102, 4.932188483229059, 5.096837184372782, 5.248400201863127, 5.305548587384595, 5.169313282763595, 4.912474152826619, 4.667122616105213, 4.433885481526182, 4.212261993076176, 4.001696038501955, 3.801653603040316, 3.611617436691254, 3.4310759586840107, 3.2595577018004116, 3.096616452033004, 2.9418267821021002, 2.794778628384035, 2.655078642406899, 2.522363194624992, 2.3962823116414955, 2.276505200377858, 2.162717087900263, 2.0546174978035006, 1.951924017610172, 1.854368245623071, 1.761685786773424, 1.6736397046959937, 1.589994711119465, 1.5105355057265881, 1.435045265482713, 1.3633334333226075, 1.2952017275600918, 1.230480546874141, 1.1689940770719431, 1.1105844423185602, 1.0550926886078398, 1.0023787143619067, 0.9522936240441326, 0.9047182900856541, 0.8595226569823872, 0.8165774417260802, 0.7757833228147628, 0.737028874272489, 0.7002077023609621, 0.6652362665965343, 0.6320098072495287, 0.6004439690950303, 0.5704585569681158, 0.5419701421357085, 0.514902074568037, 0.489190169707094, 0.46475962708548774, 0.44155163059282565, 0.41950674401237076, 0.3985655725259211, 0.37867069275114446, 0.35976921386407645, 0.3418157049591295, 0.324755079111367, 0.3085497724051944, 0.2931574626209041, 0.27853349125265825, 0.2646467929606619, 0.2514458712148386, 0.23890602982203976, 0.22699348867061042, 0.21567478673426838, 0.20492602425876177, 0.19471159120466333, 0.18500814131573837, 0.1757867981674151, 0.167027498543081, 0.15870116595445652, 0.8630562039396927, 1.2670380792590654, 1.3669087028000795, 1.475746585798198, 1.601201848366198, 1.7271281842086914, 1.8564129874616355, 1.972090959075492, 2.0739595766338024, 2.1851859094399906, 2.306096625488377, 2.4255926574799487, 2.534696325387201, 2.630064985642194, 2.7274883234648812, 2.899686104252086, 3.057559674863456, 3.406172045658247, 3.5377705059058524, 3.7327011429793338, 3.7463352127224567, 3.7587446045063775, 3.7596857436124185, 3.75979786050742, 3.7414927113632745, 3.755980003590521, 3.7420361531569357, 3.7315322744999433, 3.7577945966269968, 3.7894342809725874, 3.820664261412483, 3.8401652871571663, 3.8544533236873693, 3.8804930791109715, 3.919883814216397, 3.985223613602623, 4.087824720443854, 4.138413142548492, 4.17947179259575, 4.208370159791219, 4.227973653068137, 4.239351574113461, 4.248447418564526, 4.254967785215644, 4.264414329846159, 4.289123577419403, 4.337068502521821, 4.4352527093842555, 4.525654332871952, 4.3356999111015915, 4.119966800808643, 3.914306767837825, 3.9575036065318105, 4.017926680218281, 4.081679165177094, 4.145177909554958, 4.039382022004151, 3.847254467320226, 3.659107501503023, 3.4788018883505294, 3.588329754290669, 3.709781086362207, 3.701272169869109, 3.5559123924522753, 3.3858626340917444, 3.2206544746793417, 3.0624006301259254, 3.107193947294403, 3.1560541872542065, 3.227591077500871, 3.3199513097738986, 3.2581485284496323, 3.0997363710872854, 2.945601380485932, 2.798874363821827, 2.883921490357298, 2.9917315889732827, 3.095222325808798, 3.06051414321129, 2.9237997328713656, 2.7805301119338837, 2.8927020411669013, 2.9986297023226673, 3.0960242336326202, 3.0398201074269107, 2.9009978244466934, 2.7604553757881307, 2.6255301395020307, 2.4966762251295993, 2.3737780093333556, 2.7281421827497443, 2.6636296239357433, 2.8353484074789463, 2.9722965260835754, 3.004226672199135, 2.8966479750170575, 2.7595118948926083, 2.625556971694437, 2.4968369182319807, 2.3740290018471875, 2.257059540542479, 2.1457610516266423, 2.0399322580263592, 1.9393875229090252]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 2/2 [08:34<00:00, 164.02s/it]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 300\n",
            "\n",
            "dlosses [1.1277566, 1.0990544736385346, 1.0556731632351877, 1.0083886109739544, 0.9631030070833863, 0.9191359882702306, 0.8757713441940527, 0.8336708319333952, 0.7932278417153034, 0.7544894801736105, 0.717388494032345, 0.6819395334403571, 0.6481406121461281, 0.6159681260954095, 0.5853667614306972, 0.556267888095408, 0.5285869388900527, 0.5022557897829103, 0.47721963418345853, 0.453422160297792, 0.4308079320349851, 0.4093202472897873, 0.38891048840951536, 0.36950912812043507, 0.35107043489888434, 0.3335483703608729, 0.31693213339716725, 0.30114670828164686, 0.2861505544219025, 0.2719042082551454, 0.2583701793967261, 0.24551285198122771, 0.23329839093650429, 0.22169465294401702, 0.21067110185115412, 0.20019872831293437, 0.1902499734516256, 0.18079865633338227, 0.1718199050710511, 0.1632900913718365, 0.15518676835758263, 0.14748861149404147, 0.14017536247367735, 0.13322777590433144, 0.12662756866345282, 0.12035737178461813, 0.11440068474972517, 0.10874183206657687, 0.10336592201758597, 0.09825880747104462, 0.09340704865183035, 0.08879787777357678, 0.0844191654392359, 0.08025938872161206, 0.07630760083986941, 0.0725534023522139, 0.06898691378894116, 0.06559874965383206, 0.062379993725478415, 0.059322175593542456, 0.0564172483682033, 0.053657567504131094, 0.0510358706832625, 0.04854525870343734, 0.04617917732260343, 0.04393140001081122, 0.041796011564608625, 0.03976739254071616, 0.037840204468018314, 0.03600937579895536, 0.03427008856334556, 0.03261776568951625, 0.0310480589593784, 0.029556837565747444, 0.028140177241798035, 0.026794349934046096, 0.025515813991681756, 0.024301204846435633, 0.023147326158451817, 0.02205114140486719, 0.021009765888961796, 0.02002045914885167, 0.019080617745747052, 0.018187768412797664, 0.017339561546495744, 0.016533765023508923, 0.01576825832667144, 0.015041026964675831, 0.014350157170780002, 0.013693830866578964, 0.013070320877587978, 0.012477986388046541, 0.011915268622982176, 0.01138068674617103, 0.010872833963200441, 0.010390373819378382, 0.009932036682747426, 0.09914883545095075, 0.1455318365742925, 0.17567980314668566, 0.20425326252090653, 0.22477866106134986, 0.24242073246098775, 0.2589078391675587, 0.2725645246933116, 0.2839980369497044, 0.290093108356423, 0.2896010668311571, 0.28560261916839597, 0.2800551736567154, 0.27634067361583786, 0.2743480510888393, 0.2649338654686014, 0.2530516898727467, 0.2467207397313165, 0.23691356893893928, 0.2525694080262483, 0.287246855364804, 0.28596946512980886, 0.2866816080565993, 0.30712041845996746, 0.3469952653996705, 0.3721741075908076, 0.41116073864849256, 0.42512007039034616, 0.42261202541174525, 0.41693966822646983, 0.4139972063400525, 0.41393826463553, 0.4168033506055281, 0.4165398672521735, 0.4106556435958141, 0.4002182513484174, 0.3854334241160127, 0.3694228332944665, 0.37209531432614285, 0.36243728551183424, 0.3564890045947688, 0.3496098648086701, 0.34002344648507615, 0.32775208302780007, 0.31546218421083505, 0.30320531504572157, 0.29167990444461195, 0.28230033460201526, 0.27224424926732593, 0.2626909681993711, 0.25361535118481404, 0.24499351502098482, 0.2924271264136429, 0.2798658276883329, 0.26810085650763776, 0.2571438775848128, 0.24673474760812902, 0.23684607413027944, 0.22745183432632235, 0.2185273065125631, 0.23967541647532606, 0.23115179437349312, 0.22305435337675183, 0.2153617844298476, 0.20805384393028858, 0.20111130045570752, 0.19451588415485552, 0.22139509342325564, 0.21553475080532028, 0.21036905259064914, 0.20282287199957647, 0.19565400043805742, 0.18884357245461433, 0.1823736658703434, 0.17622725461528602, 0.20344975769396234, 0.19741929566742805, 0.19179753222652013, 0.1864568569576576, 0.18138321545223818, 0.17656325602208975, 0.19449791549137863, 0.187572091088365, 0.1813062362893293, 0.17535367423024537, 0.16969874027411563, 0.16432655301579238, 0.15922297512038527, 0.15437457611974853, 0.14976859706914364, 0.16653828012309613, 0.18246947902435098, 0.18715303658539384, 0.17927860936785353, 0.17179790351119023, 0.1646912329473601, 0.15793989591172147, 0.15152612572786478, 0.14543304405320093, 0.13964461646227028, 0.13414561025088614, 0.12892155435007122, 0.12395870124429703, 0.11924399079381155, 0.11476501586585035, 0.1105099896842872, 0.13433021120646627, 0.13120637582538602, 0.12863995938954922, 0.12620186377550427, 0.12388567294216156, 0.12168529165048599, 0.14578599935418607, 0.14230945279562404, 0.13873313483442598, 0.13533563277128782, 0.13210800581130658, 0.1290417601993244, 0.1261288268679413, 0.14654058179739426, 0.14230291415909077, 0.13767599712983952, 0.13328042595205083, 0.12910463333315159, 0.12513763034519731, 0.12136897750664076, 0.11778875731001202, 0.11438754812321472, 0.11115639939575729, 0.10808680810467274, 0.1289555573467322, 0.12472945432401819, 0.1207146564524399, 0.11690059847444051, 0.11327724339534109, 0.10983505607019665, 0.10656497811130942, 0.10345840405036656, 0.10050715869247084, 0.1221990642180066, 0.11786722242767805, 0.11375197272686595, 0.10984248551109445, 0.10612847265611151, 0.10260016044387774, 0.09924826384225564, 0.09606396207071466, 0.09303887538775071, 0.09016504303893497, 0.08743490230756001, 0.11028150498003907, 0.10603957958072015, 0.10200975045136718, 0.09818141277848186, 0.09454449198924081, 0.0910894172394618, 0.08780709622717175, 0.08468889126549621, 0.08172659655190444, 0.07891241657399226, 0.07623894559497568, 0.07369914816490994, 0.07128634060634749, 0.06899417342571317, 0.06681661460411055, 0.06474793372358807, 0.06278268688709171, 0.06091570239242017, 0.059142067122482204, 0.05745711361604114, 0.055856407784922124, 0.05433573724535906, 0.05289110023277415, 0.051518695070818486, 0.07487209805722202, 0.09705783089430538, 0.09340574253260926, 0.08993625858899795, 0.08664024884256721, 0.08350903958345801, 0.08053439078730426, 0.0777084744309582, 0.07502385389242944, 0.07247346438082712, 0.07005059434480491, 0.06774886781058383, 0.06556222760307379, 0.06348491940593926, 0.06151147661866145, 0.059636705970747536, 0.05785567385522932, 0.056163693345487015, 0.054556311861231825, 0.05302929945118939, 0.051578637661649086, 0.050200508961585795, 0.04889128669652566, 0.04764752554471854, 0.04646595245050177, 0.045343458010995846, 0.04427708829346522, 0.04326403706181112, 0.04230163839173972, 0.041387359655171894]\n",
            "\n",
            "glosses [0.79874367, 0.8278134077787399, 0.9248773415386676, 1.0504994860813022, 1.1895595757115633, 1.3415605455191248, 1.515767603921269, 1.7037651820153057, 1.8986448912830707, 2.097694341894088, 2.2983747674309143, 2.499358381392437, 2.7000648054319463, 2.9025901277061252, 3.10150106283388, 3.302110170878404, 3.5030307062263906, 3.70198886825699, 3.895633860154504, 4.081900369661428, 4.261233400265881, 4.435517592641197, 4.600864820919294, 4.76733184713102, 4.932188483229059, 5.096837184372782, 5.248400201863127, 5.305548587384595, 5.169313282763595, 4.912474152826619, 4.667122616105213, 4.433885481526182, 4.212261993076176, 4.001696038501955, 3.801653603040316, 3.611617436691254, 3.4310759586840107, 3.2595577018004116, 3.096616452033004, 2.9418267821021002, 2.794778628384035, 2.655078642406899, 2.522363194624992, 2.3962823116414955, 2.276505200377858, 2.162717087900263, 2.0546174978035006, 1.951924017610172, 1.854368245623071, 1.761685786773424, 1.6736397046959937, 1.589994711119465, 1.5105355057265881, 1.435045265482713, 1.3633334333226075, 1.2952017275600918, 1.230480546874141, 1.1689940770719431, 1.1105844423185602, 1.0550926886078398, 1.0023787143619067, 0.9522936240441326, 0.9047182900856541, 0.8595226569823872, 0.8165774417260802, 0.7757833228147628, 0.737028874272489, 0.7002077023609621, 0.6652362665965343, 0.6320098072495287, 0.6004439690950303, 0.5704585569681158, 0.5419701421357085, 0.514902074568037, 0.489190169707094, 0.46475962708548774, 0.44155163059282565, 0.41950674401237076, 0.3985655725259211, 0.37867069275114446, 0.35976921386407645, 0.3418157049591295, 0.324755079111367, 0.3085497724051944, 0.2931574626209041, 0.27853349125265825, 0.2646467929606619, 0.2514458712148386, 0.23890602982203976, 0.22699348867061042, 0.21567478673426838, 0.20492602425876177, 0.19471159120466333, 0.18500814131573837, 0.1757867981674151, 0.167027498543081, 0.15870116595445652, 0.8630562039396927, 1.2670380792590654, 1.3669087028000795, 1.475746585798198, 1.601201848366198, 1.7271281842086914, 1.8564129874616355, 1.972090959075492, 2.0739595766338024, 2.1851859094399906, 2.306096625488377, 2.4255926574799487, 2.534696325387201, 2.630064985642194, 2.7274883234648812, 2.899686104252086, 3.057559674863456, 3.406172045658247, 3.5377705059058524, 3.7327011429793338, 3.7463352127224567, 3.7587446045063775, 3.7596857436124185, 3.75979786050742, 3.7414927113632745, 3.755980003590521, 3.7420361531569357, 3.7315322744999433, 3.7577945966269968, 3.7894342809725874, 3.820664261412483, 3.8401652871571663, 3.8544533236873693, 3.8804930791109715, 3.919883814216397, 3.985223613602623, 4.087824720443854, 4.138413142548492, 4.17947179259575, 4.208370159791219, 4.227973653068137, 4.239351574113461, 4.248447418564526, 4.254967785215644, 4.264414329846159, 4.289123577419403, 4.337068502521821, 4.4352527093842555, 4.525654332871952, 4.3356999111015915, 4.119966800808643, 3.914306767837825, 3.9575036065318105, 4.017926680218281, 4.081679165177094, 4.145177909554958, 4.039382022004151, 3.847254467320226, 3.659107501503023, 3.4788018883505294, 3.588329754290669, 3.709781086362207, 3.701272169869109, 3.5559123924522753, 3.3858626340917444, 3.2206544746793417, 3.0624006301259254, 3.107193947294403, 3.1560541872542065, 3.227591077500871, 3.3199513097738986, 3.2581485284496323, 3.0997363710872854, 2.945601380485932, 2.798874363821827, 2.883921490357298, 2.9917315889732827, 3.095222325808798, 3.06051414321129, 2.9237997328713656, 2.7805301119338837, 2.8927020411669013, 2.9986297023226673, 3.0960242336326202, 3.0398201074269107, 2.9009978244466934, 2.7604553757881307, 2.6255301395020307, 2.4966762251295993, 2.3737780093333556, 2.7281421827497443, 2.6636296239357433, 2.8353484074789463, 2.9722965260835754, 3.004226672199135, 2.8966479750170575, 2.7595118948926083, 2.625556971694437, 2.4968369182319807, 2.3740290018471875, 2.257059540542479, 2.1457610516266423, 2.0399322580263592, 1.9393875229090252, 1.8437057710034954, 1.7527212128069145, 1.8913807425672644, 2.0376303966925633, 2.1694799321176395, 2.1290747021056906, 2.027957052707355, 1.9286696116712658, 2.054459746734065, 2.1887464277475814, 2.3246186499698216, 2.308636221156328, 2.2024395463094297, 2.096207456773909, 1.9939096227408175, 2.1268798234427657, 2.2657440003004123, 2.405456392112784, 2.4200811860325353, 2.3216740641488927, 2.2128433022220513, 2.106511698082447, 2.004241843640399, 1.9066971949126443, 1.813580442301116, 1.7247808367784792, 1.8753799449639692, 2.0367908536071893, 2.0912771995605577, 2.026434834495631, 1.9373538513090955, 1.8474115881474227, 1.7597846745693524, 1.6755073852868523, 1.5948650514736835, 1.7361568502458244, 1.9042144395191167, 1.9546089955582975, 1.9060997336103538, 1.8277030289713856, 1.746551520428767, 1.6668581549938764, 1.5896096396332415, 1.514957194600618, 1.4436224173212153, 1.3755068763761618, 1.6202370390728567, 1.807376470383374, 1.915528235739022, 1.9353895316623368, 1.8925574365851863, 1.8290530002532108, 1.7619996248281813, 1.6941052068935807, 1.6258029153132065, 1.5581167459164504, 1.4919784688642344, 1.4273600272840443, 1.3644861989101642, 1.3032376112775832, 1.2441905145967989, 1.187321330345545, 1.1325240708639523, 1.0797556296872879, 1.0293563674483097, 0.9811709105535076, 0.9349802900066514, 0.8908398806080233, 0.848649959993736, 0.8084557084200354, 1.3080455687204693, 1.545976286652854, 1.7453160993725183, 1.874338813309532, 1.923187635640729, 1.9113574655705758, 1.8650754273197419, 1.799645890916218, 1.7296929191734274, 1.658811720175205, 1.5884830672189845, 1.5196423861099273, 1.4526814424571732, 1.3874906895953887, 1.3248449383654741, 1.2645784944350142, 1.2066893725148724, 1.1511404047284814, 1.097969750650781, 1.0470280153887022, 0.9981844977209687, 0.95166478036704, 0.9072330017131489, 0.8648926719556078, 0.8243237512114975, 0.7857166755783723, 0.7487306948412217, 0.7135476103661774, 0.6799447370992504, 0.647921621725468]\n",
            "\n",
            "gans after task 1: [GAN_generator(\n",
            "  (generator_net): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=256, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.2)\n",
            "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
            "  )\n",
            "), GAN_generator(\n",
            "  (generator_net): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=256, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.2)\n",
            "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
            "  )\n",
            ")]\n",
            "solvers after task 1: [Solver(\n",
            "  (l1): BayesLinear()\n",
            "  (l2): BayesLinear()\n",
            "  (l3): BayesLinear()\n",
            "  (relu): ReLU()\n",
            "), Solver(\n",
            "  (l1): BayesLinear()\n",
            "  (l2): BayesLinear()\n",
            "  (l3): BayesLinear()\n",
            "  (relu): ReLU()\n",
            ")]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT9tnUOKFyZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}